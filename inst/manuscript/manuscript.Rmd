---
documentclass: jss
author:
    # use this syntax to add text on several lines
    # To add another line, use \AND at the end of the previous one as above
    # use a different affiliation in adress field (differently formated here)
  - name: Michael Dumelle
    orcid: 0000-0002-3393-5529
    affiliation: |
      | United States
      | Environmental Protection Agency
    address: |
      | 200 SW 35th St
      | Corvallis, OR, 97330
    email: \email{Dumelle.Michael@epa.gov}
  - name: Jay M. Ver Hoef
    orcid: 0000-0003-4302-6895
    affiliation: |
      | Alaska Fisheries
      | Science Center
  - name: Matt Higham
    orcid: 0009-0006-4217-625X
    affiliation: |
      | St. Lawrence University
title:
  # If you use tex in the formatted title, also supply version without
  # For running headers, if needed
  formatted: "Spatial Generalized Linear Models in \\proglang{R} Using \\pkg{spmodel}"
  plain:     "Spatial Generalized Linear Models in R Using spmodel"
  short:     "Spatial Generalized Linear Models in \\proglang{R} Using \\pkg{spmodel}"
abstract: |
  Generalized linear models (GLMs) describe a non-normal response variable that may be binary, count, skewed, or a proportion. Typically, observations in a GLM are assumed independent of one another. For spatial data, this independence assumption is impractical, as nearby locations tend to be more similar than locations far apart. The \pkg{spmodel} \proglang{R} package provides tools to fit GLMs that incorporate spatial correlation (i.e., spatial generalized linear models, or SPGLMs). SPGLMs are fit in \pkg{spmodel} using a novel application of the Laplace approximation via \code{spglm()} for point-referenced data or \code{spgautor()} for areal (i.e., lattice), data. \code{spglm()} and \code{spgautor()} closely resemble [glm]{.fct} from base \proglang{R} but include arguments that control the spatial correlation structure. \pkg{spmodel} has many helper functions for model inspection and diagnostics, some of which leverage other \proglang{R} packages like [broom]{.pkg} and [emmeans]{.pkg}. \pkg{spmodel} has tools to make predictions of the latent spatial-mean process at unobserved locations. \pkg{spmodel} also provides many advanced features like accommodating geometric anisotropy and nonspatial random effects, simulating spatially autocorrelated data, and more. Here we use \pkg{spmodel} to illustrate the modeling of binary, count, skewed and proportion response variables from several point-referenced and areal data sets. 
keywords:
  # at least one keyword must be supplied
  formatted: [autoregressive model, geostatistical model, spatial covariance, spatial correlation]
  plain:     [autoregressive model, geostatistical model, spatial covariance, spatial correlation]
preamble: |
  \usepackage{amsmath,amsfonts,amssymb}
  \usepackage{bm, bbm}
  \usepackage{lineno}
  \usepackage{caption, subcaption}
output: rticles::jss_article
editor_options: 
  chunk_output_type: console
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
library(spmodel)
library(ggplot2)
library(dplyr)
library(emmeans)
library(here)
fig_path <- here("inst", "manuscript", "figures")
```

\newpage

# Introduction {#sec-intro}

In practice, non-Gaussian data are ubiquitous. Non-Gaussian that belong to an exponential family data can be naturally modeled using a generalized linear model (GLM) regression framework [@nelder1972generalized; @mccullagh1989generalized; @myers2012generalized; @faraway2016extending]. In a GLM, an $n \times 1$ response variable $\mathbf{y}$ belongs to a a statistical distribution (e.g., Poisson, Binomial) with some mean and variance. Often, the analysis goal is to study the impact of a linear function of several explanatory variables on $\text{y}$ through a GLM. In this context, the latent (i.e., unobserved) mean of $\mathbf{y}$, $\boldsymbol{\mu}$, is linked to these explanatory variables via a link function:
\begin{equation}\label{eq-glm}
f(\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta}) \equiv \mathbf{w} = \mathbf{X} \boldsymbol{\beta},
\end{equation}
where for a sample size $n$, $f(\cdot)$ is a link function that connects $\boldsymbol{\mu}$ to $\mathbf{w}$, $\mathbf{X}$ is the $n \times p$ design matrix of explanatory variables, and $\boldsymbol{\beta}$ is the $p \times 1$ vector of fixed effects. While the mean is typically constrained in some way (e.g., between zero and one if a probability), the link function generally makes $\mathbf{w}$ is unconstrained. Common link functions inlude the log odds (i.e., logit) link for binary and proprtion data and the log link count and skewed data. Equation$~$\ref{eq-glm} can also be written in terms of the inverse link function, $f^{-1}(\cdot)$:
\begin{equation}\label{eq-glm2}
\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta} \equiv f^{-1}(\mathbf{w}) = f^{-1}(\mathbf{X} \boldsymbol{\beta}),
\end{equation}

The GLM fixed effects ($\boldsymbol{\beta}$) are typically estimated via maximum likelihood [@chambers1992S]. It is often convenient to compute the maximum likelihood estimates using the iteratively reweighted least squares (IRWLS) algorithm [@wood2017generalized], which is the approach used by the  \code{glm()} function in the \proglang{R} programming language [@rcore2024]. GLMs add an additional layer of complexity compared to linear regression models, as the left-hand size of Equation$~$\ref{eq-glm} is a function of the mean of $\mathbf{y}$ rather than $\mathbf{y}$ itself (as in linear regression models).

The standard GLM assumes the elements of $\mathbf{y}$ are independent. This independence assumption is typically impractical for spatial data. In spatial data, nearby observations tend to be more similar than distant observations [@tobler1970computer], leading to positive spatial covariance among observations. The consequences of ignoring spatial covariance in statistical models for spatial data can be severe and include imprecise parameter estimates as well as misleading standard errors that inflate Type-I error rates and decrease power [@zimmerman2024spatial].

An approach for handling spatial data using a GLM is to assume $\mathbf{w}$ has spatial covariance. This is achieved by adding to Equation$~$\ref{eq-glm} two random effects, $\boldsymbol{\tau}$ and $\boldsymbol{\epsilon}$. The random effect $\boldsymbol{\tau}$ is an $n \times 1$ column vector of spatially dependent random errors. We assume that $\text{E}(\boldsymbol{\tau}) = \boldsymbol{0}$ and $\text{Cov}(\boldsymbol{\tau}) = \sigma^2_\tau \mathbf{R}$, where $\text{E}(\cdot)$ and $\text{Cov}(\cdot)$ denote expectation and covariance, respectively. The variance parameter $\sigma^2_\tau$ controls the magnitude of spatial covariance and is often called a partial sill, while the matrix $\mathbf{R}$ is an $n \times n$ spatial correlation matrix that depends on a range parameter controls the distance-decay rate of the spatial correlation. One example of a spatial covariance matrix is the "exponential", which is given by
\begin{equation}\label{eq-spcov-exp}
  \text{Cov}(\boldsymbol{\tau}) = \sigma^2_{de} \exp(-\mathbf{H}/\phi),
\end{equation}
where $\mathbf{H}$ is a matrix of pairwise distances among the elements of $\mathbf{y}$ and $\phi$ is a range parameter. From Equation$~$\ref{eq-spcov-exp}, as the distance between two elements of $\mathbf{y}$ increases, the spatial covariance decreases, which reflects intuition. Moreover, as the range parameter, $\phi$, increases, the strength of spatial dependence increases (Figure$~$\ref{fig-range}). The random effect $\boldsymbol{\epsilon}$ is an $n \times 1$ column vector of independent random errors. We assume that $\text{E}(\boldsymbol{\epsilon}) = \boldsymbol{0}$ and $\text{Cov}(\boldsymbol{\tau}) = \sigma^2_\epsilon \mathbf{I}$, where $\mathbf{I}$ is an $n \times n$ identity matrix. The variance parameter $\sigma^2_\epsilon$ controls the magnitude of nonspatial variability (i.e., fine-scale variation) and is often called a nugget. 

\begin{figure}
\centering
\includegraphics[width = 0.7\linewidth]{figures/figure-01.png}
\caption{An exponential spatial correlation function with varying range parameters.}
\label{fig-range}
\end{figure}

Through inclusion of $\boldsymbol{\tau}$ and $\boldsymbol{\epsilon}$, the spatial GLM (SPGLM) can be written as
\begin{equation}\label{eq-spglm}
f(\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{\epsilon}) \equiv \mathbf{w} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon}.
\end{equation}
Often in spatial statistics, quantities are explicitly referenced with respect to $\mathbf{s}$, a vector of coordinates indexing the observation [@cressie1993statistics]. For example, $\mathbf{y}$ and $\mathbf{X}$ may instead be written $\mathbf{y}(\mathbf{s})$ and $\mathbf{X}$, respectively. We acknowledge the utility of this nomenclature but drop the explicit dependence on $\mathbf{s}$ for simplicity of notation.
Assuming independence among $\boldsymbol{\tau}$ and $\boldsymbol{\epsilon}$, it follows that 
\begin{equation}\label{eq-spcov}
 \text{Cov}(\boldsymbol{\tau} + \boldsymbol{\epsilon}) = \text{Cov}(\boldsymbol{\tau}) + \text{Cov}(\boldsymbol{\epsilon}) = \sigma^2_{\tau}\mathbf{R} + \sigma^2_{\epsilon} \mathbf{I}.
\end{equation}
To better align with intuition, we henceforth $\sigma^2_{\tau}$ as $\sigma^2_{de}$ (for spatial error variance) and $\sigma^2_{\epsilon}$ as $\sigma^2_{ie}$ (for independent error variance). The parameters $\sigma^2_{de}$, $\sigma^2_{ie}$, the range parameter $\phi$ in $\mathbf{R}$, and any other parameters in $\mathbf{R}$ compose $\boldsymbol{\theta}$, the covariance parameter vector. 

Fitting and using SPGLMs is challenging both conceptually and computationally [@bolker2009generalized]. Recently, however, there have been numerous, significant advances in \proglang{R} software that have made these models more accessible to practitioners. The \pkg{brms} [@burkner2017brms], \pkg{carBayes} [@lee2013carbayes], \pkg{ngspatial} [@hughes2020ngspatial], \pkg{R-INLA} [@lindgren2015bayesian] and \pkg{inlabru} [@bachl2019inlabru], \pkg{spBayes} [@finley2007spbayes], \pkg{spOccupancy} [@doser2022spoccupancy], \pkg{spAbundance} [@doser2024spabundance], and \pkg{spNNGP} [@finley2002spnngp] packages take a Bayesian approach, either directly sampling from posterior distributions of parameters (e.g., using MCMC) or approximating them. A benefit of Bayesian approaches is that prior information can be incorporated and uncertainty quantification of parameter estimates is straightforward. However, Bayesian approaches, especially those using MCMC, can be computationally expensive. In order to reduce computation time, many of these packages work with the precision matrix instead of the covariance matrix so that computationally expensive matrix inversion is not required. For example, \pkg{R-INLA} uses the precision matrix and tends to be very fast. Working with precision matrices, however, can be more restrictive and less intuitive than working directly with the covariance matrix. The [FRK]{.pkg} [@sainsbury2024modeling], \pkg{glmmTMB} [@brooks2017glmmtmb], \pkg{hglm} [@ronnegard2010hglm], \pkg{mgcv} [@wood2017generalized], and \pkg{spaMM} [@rousset2014spamm] packages directly use Laplace, quasi-likelihood, or reduced-rank approaches to estimate parameters. These direct approaches tend to be computationally efficient, as they don't rely on MCMC sampling. In contrast to the Bayesian approach, a drawback of these direct approaches is that prior information cannot be formally incorporated and covariance parameter uncertainty is more challenging to quantify. The \pkg{sdmTMB} [@anderson2024sdmtmb] package combines elements of \pkg{R-INLA}, \pkg{glmmTMB}, and properties of Gaussian Markov random fields to fit a wide variety of SPGLMs, and \pkg{tinyVAST} [@thorson2025tinyVAST] extends some of these models to multivariate or (dynamic) structural equation models.

@ver2024marginal proposed a novel approach to fitting SPGLMs that leverages the Laplace approximation while marginalizing over both the latent $\mathbf{w}$ and the fixed effects ($\boldsymbol{\beta}$) and accommodating spatial covariance. @ver2024marginal showed that this approach performed efficiently in a variety of simulation settings, generally having appropriate confidence interval coverage for the fixed effects and prediction interval coverage for new $\mathbf{w}$. The approach performed similarly to the Bayesian SPGLM approach in \pkg{spBayes} and the automatic differentiation SPGLM approach in \pkg{glmmTMB} but was much faster. At small sample sizes, the approach outperformed the approximate Bayesian SPGLM approach in \pkg{R-INLA} and had similar computational times. For moderate sample sizes, it performed similarly to \pkg{R-INLA}, though \pkg{R-INLA} was faster. This novel approach is particularly attractive for two reasons. First, it is general enough that can be applied to any covariance structure (not just spatial). Second, after estimating the covariance parameters, analytical solutions exist for the fixed effects (and their standard errors) as well as predictions of the latent $\mathbf{w}$ at new locations (and their standard errors). The \pkg{spmodel} \proglang{R} package [@dumelle2023spmodel] recently provided full support for the methods in @ver2024marginal applied to binary, count, skewed, and proportion data for over 20 different spatial covariance types.

The \pkg{spmodel} \proglang{R} package [@dumelle2023spmodel] recently provided a full set of modeling tools for SPGLMs fit using the methods described in @ver2024marginal. These modeling tools are approachable and mirror the familiar \code{glm()} syntax from base-\proglang{R}, making the transition from GLMs to SPGLMs relatively seamless. The \code{spglm()} function fits SPGLMs for point-referenced data (e.g., x-coordinates and y-coordinates representing point locations in a field), while the \code{spgautor()} function fits SPGLMs for areal data (e.g., polygon boundaries representing geographic subsets of a region). \pkg{spmodel} supports the binomial distribution for binary data, Poisson and negative binomial distributions for count data, Gamma and inverse Gaussian distributions for skewed data, and the beta distribution for proportion data. There are 20 different spatial covariance structures available including the exponential, Gaussian, and spherical for point-referenced data (Figure$~$\ref{fig-type}) and the conditional autoregressive, and simultaneous autoregressive structures for areal data. \pkg{spmodel} provides tools for commonly used model summaries, visualizations, and diagnostics (e.g., Cook's distance) using standard \proglang{R} helper functions like \code{summary()}, \code{plot()}, and \code{cooks.distance()}. \pkg{spmodel} also provides tools to predict $\mathbf{w}$ at new locations and quantify uncertainty in those prediction using \code{predict()}. This core functionality, combined with several advanced features we describe throughout the manuscript, enable \pkg{spmodel} to provide some novel and important capabilities previously missing from the existing SPGLM ecosystem in \proglang{R}.

\begin{figure}
\centering
\includegraphics[width = 0.7\linewidth]{figures/figure-02.png}
\caption{Exponential, Gaussian, and spherical spatial correlation functions all with range parameters equal to 0.5.}
\label{fig-type}
\end{figure}

\pkg{spmodel} (version 0.11.0) is arguably most similar to \pkg{sdmTMB} (version 0.7.4) in terms of scope and feel. Both packages use similar syntax as \code{glm()}, accommodate flexible \code{formula} arguments (e.g., offsets, splines), handle spatial covariance that decays at different rates in different rates (i.e., geometric anisotropy), incorporate nonspatial random effects, support other \proglang{R} packages for modeling like \pkg{broom} [@robinson2021broom; @kuhn2022tidy] and \pkg{emmeans} [@lenth2024emmeans], and have tools for model summaries, prediction, and simulating data. There are some notable differences between the two packages, however. \pkg{sdmTMB} supports several additional GLM distributions like the Tweedie, supports Hurdle models, and can incorporate prior information through Bayesian applications. \pkg{sdmTMB} also provides tools for working with temporal data and enhanced visualizations of marginal effects. \pkg{sdmTMB} does require a preprocessing step of constructing a mesh for the stochastic partial differential equation approach, and the density of the mesh can affect model results and computational complexity. \pkg{spmodel} does not require the construction of a mesh prior to modeling. \pkg{spmodel} supports 20 different spatial covariances and models them directly, rather than using a precision matrix approximation to the MatÃ©rn spatial covariance as in \pkg{sdmTMB}. \pkg{spmodel} also provides experimental design tools (e.g., analysis of variance, contrasts), supports \pkg{sf} objects in modeling and prediction functions [@pebesma2018sf], has several specialized model diagnostics like leverage values and Cook's distances, and has analytic solutions for prediction standard errors. Other similarities and differences do exist between \pkg{sdmTMB} and \pkg{spmodel}, and both packages continue to evolve. Overall, we believe that these packages are complementary and enhance the suite of SPGLM tools accessible to practitioners.

The rest of this article is organized as follows. In Section$~$\ref{sec-spglm}, we provide some background for the SPGLM fitting and prediction routines in \pkg{spmodel}. In Section$~$\ref{sec-applications}, we provide several applications of \pkg{spmodel} to spatial binary, count, skewed and proportion data with both point-referenced and areal supports. And in Section$~$\ref{sec-discussion}, we end with a discussion synthesizing \pkg{spmodel}'s contributions to the analysis of SPGLMs in \proglang{R}.

# The spatial generalized linear model and marginalizatoin {#sec-spglm}

\pkg{spmodel} implements the novel methods described in @ver2024marginal to fit SPGLMs, which leverages the Laplace approximation and marginalizes over both the latent $\mathbf{w}$ and the fixed effects while accommodating spatial covariance. A beneficial aspect of this approach is that it formally maximizes a hierarchical GLM likelihood [@lee1996hierarchical; @wood2017generalized]. This makes likelihood-based statistics for model comparison like AIC [@akaike1974new], AICc [@hoeting2006model], BIC [@schwarz1978estimating], deviance [@mccullagh1989generalized], and likelihood ratio tests available. These types of statistics are not available for quasi-likelihood [@wedderburn1974quasi; @breslow1993approximate] or pseudo-likelihood approaches [@wolfinger1993generalized], which only specify the first two moments of a distribution. @ver2024marginal provides thorough details regarding the method and contextualizes its development which built upon similar methods [@evangelou2011estimation, @bonat2016practical]. Next, we describe a brief overview of the approach and how it can be used for parameter estimation, inference, and prediction.

## Formulating the hierarchical likelihood

We can write the SPGLM likelihood hierarchically as
\begin{equation}\label{eq-marginal}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} \int_{\boldsymbol{\beta}} [\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] d\boldsymbol{\beta} d\mathbf{w},
\end{equation}
where $[\mathbf{y} | f^{-1}(\mathbf{w}), \varphi]$ is the density for the appropriate response distribution of $\mathbf{y}$ (e.g., binomial, Poisson) given the latent $\mathbf{w}$ and dispersion parameter ($\varphi$), and $[\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}]$ is the multivariate Gaussian density for $\mathbf{w}$ given the explanatory variables ($\mathbf{X}$), fixed effects ($\boldsymbol{\beta}$), and spatial covariance parameters ($\boldsymbol{\theta}$). The elements of $[\mathbf{y} | f^{-1}(\mathbf{w}), \varphi]$ are conditionally independent (given $\mathbf{w}$), but the elements of $[\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}]$ share spatial covariance. Following @harville1977maximum, we can integrate $\boldsymbol{\beta}$ out of Equation$~$\ref{eq-spglm}, which yields
\begin{equation}\label{eq-marginal2}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} [\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] d\mathbf{w},
\end{equation}
where $[\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}]$ is the restricted (i.e., residual) multivariate Gaussian density [@patterson1971recovery] for $\mathbf{w}$ given the explanatory variables and covariance parameters. Equation$~$\ref{eq-marginal2} can be synonymous written after profiling the overall variance out of $\boldsymbol{\Sigma}$, which reduces the dimension of $\boldsymbol{\theta}$ by one for optimization [@wolfinger1994computing]. The restricted multivariate Gaussian density is given by
\begin{equation}\label{eq-reml-def}
[\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] = \frac{\exp(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\tilde{\boldsymbol{\beta}}) \boldsymbol{\Sigma}^{-1} (\mathbf{y} - \mathbf{X}\tilde{\boldsymbol{\beta}})^T)}{(2 \pi)^{(n - p)/2} |\boldsymbol{\Sigma}|^{1/2}|\mathbf{X}^T \boldsymbol{\Sigma}^{-1} \mathbf{X}|^{1/2}},
\end{equation}
where $\tilde{\boldsymbol{\beta}} = (\mathbf{X}^T \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\Sigma}^{-1} \mathbf{w}$ and $|\cdot|$ denotes the determinant. Next, let 
\begin{equation}\label{eq-marginal03}
  \ell_\mathbf{w} = \log([\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}])
\end{equation}
and rewrite Equation$~$\ref{eq-marginal2} as 
\begin{equation}\label{eq-marginal3}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} \exp(\ell_\mathbf{w}) d\mathbf{w}.
\end{equation}
A second-order Taylor series expansion of $\ell_\mathbf{w}$ around $\hat{\mathbf{w}}$ yields
\begin{equation}\label{eq-marginal4}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx \int_{\mathbf{w}} \exp(\ell_{\hat{\mathbf{w}}} + \mathbf{g}^T(\mathbf{w} - \hat{\mathbf{w}}) + \frac{1}{2}(\mathbf{w} - \hat{\mathbf{w}})^T \mathbf{G} (\mathbf{w} - \hat{\mathbf{w}}))d\mathbf{w},
\end{equation}
where $\mathbf{g}$ and $\mathbf{G}$ are the gradient and Hessian, respectively, of $\ell_\mathbf{w}$ with respect to $\mathbf{w}$. If $\hat{\mathbf{w}}$ is a value for which $\mathbf{g} = \mathbf{0}$,
\begin{equation}\label{eq-marginal5}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx \exp(\ell_{\hat{\mathbf{w}}}) \int_{\mathbf{w}} \exp(-\frac{1}{2}(\mathbf{w} - \hat{\mathbf{w}})^T (-\mathbf{G}) (\mathbf{w} - \hat{\mathbf{w}}))d\mathbf{w}.
\end{equation}
The integral in Equation$~$\ref{eq-marginal5} can be solved by leveraging properties of the normalizing constant of a multivariate Gaussian distribution. Thus, rewriting $\exp(\ell_{\hat{\mathbf{w}}})$ yields
\begin{equation}\label{eq-marginal6}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx [\mathbf{y} | f^{-1}(\hat{\mathbf{w}}), \varphi] [\hat{\mathbf{w}} | \mathbf{X}, \boldsymbol{\theta}] (2 \pi)^{n/2}|-\mathbf{G}_{\hat{\mathbf{w}}}|^{-1/2}.
\end{equation}

Maximizing the natural logarithm of Equation$~$\ref{eq-marginal6} requires a doubly iterative process over $\boldsymbol{\theta}$ and $\varphi$ as well as $\mathbf{w}$, eventually yielding the the marginal restricted maximum likelihood estimators $\hat{\varphi}$ and $\hat{\boldsymbol{\theta}}$ and their corresponding values of $\hat{\mathbf{w}}$. Maximizing this log likelihood is a computationally expensive operation that involves repeatedly evaluating $\boldsymbol{\Sigma}^{-1}$, $\mathbf{g}$, and $\mathbf{G}$; see @ver2024marginal for more details and forms of $\mathbf{g}$ and $\mathbf{G}$ for various response distributions.

## Estimating fixed effects

Though the fixed effects are integrated out of the likelihood, we can still estimate them using generalized least squares (GLS) principles, a common practice for linear models estimated using restricted maximum likelihood methods. Had we observed $\mathbf{w}$, a GLS estimator for $\boldsymbol{\beta}$ is given by
\begin{equation}\label{eq-gls1}
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{w} = \mathbf{B}\mathbf{w},
\end{equation}
where $\mathbf{B} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}$. However, we only observe $\hat{\mathbf{w}}$, so it is reasonable to define $\hat{\boldsymbol{\beta}} = \mathbf{B}\hat{\mathbf{w}}$. Thus, to derive properties of $\hat{\boldsymbol{\beta}}$ like expectation and variance, we must derive these properties for $\hat{\mathbf{w}}$. To do so, we must condition on $\mathbf{w}$ as if it were observed and invoke properties of the laws of total expectation and variance. Because $\hat{\mathbf{w}}$ was optimized via the likelihood, we assume that given $\mathbf{w}$, $\hat{\mathbf{w}}$ has mean $\mathbf{w}$ and variance approximately equal to $-\mathbf{H}^{-1}$ (the inverse Hessian). It follows that $\text{E}(\hat{\mathbf{w}})$ is given by
\begin{equation}
  \text{E}(\hat{\mathbf{w}}) = \text{E}(\text{E}(\hat{\mathbf{w}} | \mathbf{w}))
   = \text{E}(\mathbf{w}) 
   = \mathbf{X}\boldsymbol{\beta}
\end{equation}
and $\text{Var}(\hat{\mathbf{w}})$ is given by
\begin{align}
  \text{Var}(\hat{\mathbf{w}}) & = \text{E}(\text{Var}(\hat{\mathbf{w}} | \mathbf{w})) + \text{Var}(\text{E}(\hat{\mathbf{w}} | \mathbf{w})) \\
  & = \text{E}(-\mathbf{H}^{-1}) + \text{Var}(\mathbf{w})\\
  & = -\mathbf{H}^{-1} + \boldsymbol{\Sigma}
\end{align}
Putting this all together, it follows that
\begin{equation}
  \text{E}(\hat{\boldsymbol{\beta}})  = \text{E}(\mathbf{B}\hat{\mathbf{w}}) 
  = \mathbf{B}\text{E}(\hat{\mathbf{w}}) =  (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})\boldsymbol{\beta} = \boldsymbol{\beta}
\end{equation}
and
\begin{align}
  \text{Var}(\hat{\boldsymbol{\beta}}) & = \text{Var}(\mathbf{B}\hat{\mathbf{w}}) \\
  & = \mathbf{B} \text{Var}(\hat{\mathbf{w}}) \mathbf{B}^\top\\
  & = \mathbf{B} (-\mathbf{H}^{-1} + \boldsymbol{\Sigma}) \mathbf{B}^\top \\
  & = \mathbf{B}-\mathbf{H}^{-1}\mathbf{B}^\top + \mathbf{B}\boldsymbol{\Sigma}\mathbf{B}^\top \\
  & = \mathbf{B}-\mathbf{H}^{-1}\mathbf{B}^\top + (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}
\end{align}
In practice, $\text{Var}(\hat{\boldsymbol{\beta}})$ is estimated by evaluating $\boldsymbol{\Sigma}$ at $\hat{\boldsymbol{\theta}}$, the estimated covariance parameter vector.

These results are important because they justify closed-form solutions for $\hat{\boldsymbol{\beta}}$ and its associated variance. Closed-form solutions are useful because they bypass the need for computationally expensive sampling-based strategies to evaluate the mean and variance of $\hat{\boldsymbol{\beta}}$ -- a common technique for other approaches to SPGLMs like Bayesian MCMC. 

## Inspecting model diagnostics

Inspecting model diagnostics is an important step of the modeling process that can yield valuable insights into model behavior and unusual observations. @montgomery2021introduction contextualize three components of unusual observations: outliers, leverage, and influence. An observation is an outlier if it has an unusual response value relative to expectation. The response GLM residuals simply compare the observation to its fitted latent mean:
\begin{equation}
  \mathbf{r}_{r} = \mathbf{y} - f^{-1}(\hat{\mathbf{w}})
\end{equation}
Because observations often have a unique support in a GLM (e.g., only two possible response values for binary data) and the variance of an observation generally depends on its mean, response residuals lack some utility. Deviance residuals are a function of response residuals that are appropriately scaled to behave more like response residuals in a standard linear model. Deviance residuals are given by
\begin{equation}
  \mathbf{r}_{d} = sign(\mathbf{r}_{r})\sqrt{\mathbf{d}},
\end{equation}
where $\mathbf{d}$ is a vector of individual deviances. The sum of the squared deviance residuals equals the sum of $\mathbf{d}$. The sum of $\mathbf{d}$ is the deviance of the model fit, which quantifies twice the difference in log likelihoods between the a saturated model that fits every observation perfectly (i.e., $\mathbf{y} = f^{-1}(\hat{\mathbf{w}}_i)$ for all $i$) and the fitted model. Deviance is often used as a fit statistic; lower values of deviance imply a better model fit. Pearson and standardized residuals are other types of GLM residuals that involve some scaling of the response residuals; the Pearson residuals scale $\mathbf{r}_{r}$ by the squrae root of $\mathbf{V}$, while the standardized residuals scale the deviance residuals by $\frac{1}{\sqrt{(1 - \mathbf{L}_{ii})}}$, where $\mathbf{L}_{ii}$ is the $i$th diagonal element of the leverage matrix, which we discuss next.
An observation has high leverage if its combination of explanatory variables is far away from other observations. In a linear model, the leverage values are the diagonal of the leverage (i.e., projection, hat) matrix, $\mathbf{L} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top$. In a GLM, the leverage matrix is given by
\begin{equation}
  \mathbf{L} = \mathbf{V}^{1/2} \mathbf{X} (\mathbf{X}^\top \mathbf{V} \mathbf{X}) \mathbf{X}^\top \mathbf{V}^{1/2},
\end{equation}
where $\mathbf{V}$ is a diagonal matrix with $i$th diagonal element equal to the variance of the response distribution evaluated at $f^{-1}(\mathbf{w}_i)$ [@faraway2016extending]. The matrix $\mathbf{V}$ is sometimes called the GLM weight matrix. The larger the $i$th diagonal element of the hat matrix, the more severe the leverage from the $i$th observation.
An observation is influential if it has a sizeable impact on model fit. Influence is measured using Cook's distance [@cook1979influential; @cook1982residuals], which is given for a GLM by
\begin{equation}
  \mathbf{c} = \mathbf{r}^2_{s} \frac{diag(\mathbf{L})}{\text{tr}(\mathbf{L})(\mathbf{1} - diag(\mathbf{L}))},
\end{equation}
where $\mathbf{r}^2_{s}$ are the standardized residuals and $diag(\mathbf{L})$ indicates the diagonal elements of the leverage matrix. The larger the $i$th diagonal element of the hat matrix, the more severe the influence from the $i$th observation.  @montgomery2021introduction provide guidance for interpreting these types of statistics, including cutoffs to consider when identifying unusual residual, leverage, or influence values.

In a linear model, the $R^2$ (R-squared) statistic quantifies the proportion of variability in the data captured by the explanatory variables and is calculated as one minus the ratio of the error sum of squares to the total sum of squares [@rencher2008linear]. In a GLM, there are many ways to define such a statistic [@smith2013comparison]. One such approach is to use one minus the deviance ratio:
\begin{equation}
  PR^2 = 1 - \frac{deviance_{fit}}{deviance_{null}},
\end{equation}
where $deviance_{fit}$ is the deviance of the fitted model (sometimes called the residual deviance) and $deviance_{null}$ is the deviance of the model taking $\mathbf{X} \equiv \mathbf{1}$, a column of all ones (i.e., an intercept-only model). In practice, $deviance_{null}$ is derived by computing $\hat{\mathbf{w}}$ when $\mathbf{X} \equiv \mathbf{1}$ given $\hat{\boldsymbol{\theta}}$ and $\hat{\varphi}$ from the fitted model. Like the standard $R^2$, this statistic attempts to capture variability (i.e., deviance) attributable to the explanatory variables. Because the $deviance_{null}$ denominator changes across fitted models (as the values of $\hat{\boldsymbol{\theta}}$ and $\hat{\varphi}$ change), this statistic should not be used as a model comparison tool. Instead, it should be used as an informative diagnostic tool unique to each model fit.
## Predicting at new locations

We may also predict values of the latent mean (on the link scale) at new locations by leveraging the spatial covariance between observed locations and new locations (spatial prediction is also called Kriging; see @cressie1990origins). Again suppose that we observed $\mathbf{w}$ and we want to make predictions at $\mathbf{u}$, a vector of latent means at the new locations that follows the same SPGLM from Equation~\ref{eq-spglm} with fixed effects design matrix, $\mathbf{X}_{\mathbf{u}}$. The vector $(\mathbf{w}, \mathbf{u})^\top$ has the following properties:
\begin{align}
  \text{E}(\mathbf{w}, \mathbf{u})^\top & = (\text{E}(\mathbf{w}), \text{E}(\mathbf{u}))^\top = (\mathbf{X}\boldsymbol{\beta}, \mathbf{X}_\mathbf{u}\boldsymbol{\beta})^\top \\
  \text{Var}(\mathbf{w}, \mathbf{u})^\top & = \begin{bmatrix} \text{Var}(\mathbf{w}, \mathbf{w}) & \text{Var}(\mathbf{w}, \mathbf{u}) \\ \text{Var}(\mathbf{u}, \mathbf{w}) & \text{Var}(\mathbf{u}, \mathbf{u}) \end{bmatrix} = \begin{bmatrix} \boldsymbol{\Sigma} & \boldsymbol{\Sigma}_{\mathbf{w}\mathbf{u}} \\ \boldsymbol{\Sigma}_{\mathbf{u}\mathbf{w}} & \boldsymbol{\Sigma}_{\mathbf{u}\mathbf{u}} \end{bmatrix}
\end{align}
Because we have observed $\mathbf{w}$, we may derive the conditional distribution of $\mathbf{u}|\mathbf{w}$, which has the following properties:
\begin{align}
  \text{E}(\mathbf{w} | \mathbf{u}) & = \mathbf{X}_{\mathbf{u}} \boldsymbol{\beta} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\mathbf{w} - \mathbf{X}\boldsymbol{\beta}) \\
  \text{E}(\mathbf{w} | \mathbf{u}) & = \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}_{\mathbf{w}, \mathbf{u}}
\end{align}
@ver2024marginal show how these equations are adjusted to reflect uncertainty in both $\hat{\boldsymbol{\beta}}$ and $\hat{\mathbf{w}}$ while leveraging the laws of total expectation and variance yet again. They derive the predictor of $\mathbf{u}$, $\hat{\mathbf{u}}$, and its associated variance, given by:
\begin{align}
  \hat{\mathbf{u}} & = \mathbf{X}_{\mathbf{u}} \hat{\boldsymbol{\beta}} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\hat{\mathbf{w}} - \mathbf{X}\hat{\boldsymbol{\beta}}) \\
  \text{Var}(\hat{\mathbf{u}}) & = \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}_{\mathbf{w}, \mathbf{u}} + \mathbf{K}(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{K}^\top + \boldsymbol{\Lambda}(-\mathbf{H})^{-1}\boldsymbol{\Lambda}^\top,
\end{align}
where $\mathbf{K} = \mathbf{X}_{\mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \mathbf{X}$ and $\boldsymbol{\Lambda} = \mathbf{X}_{\mathbf{u}}\mathbf{B} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\mathbf{1} - \mathbf{X}\mathbf{B})$ for a vector of ones, $\mathbf{1}$.

As with $\hat{\boldsymbol{\beta}}$, in practice these covariance matrices are evaluated at $\hat{\boldsymbol{\theta}}$. Moreover, these closed-form solutions provided enhance computational efficiency and clarity of the predictor's behavior. 

# Modeling moose presence in Alaska, USA {#sec-applications}

The \code{moose} data in \pkg{spmodel} contain information on moose (Alces Alces) presence in the Togiak region of Alaska, USA.   \code{moose}is an \code{sf} object, a special data frame that is supplemented with spatial information using the \pkg{sf} package in \proglang{R} [@pebesma2018sf]. The first few rows of \code{moose} look like:

```{r}
head(moose)
```

There are five columns: \code{elev}, the numeric site elevation (meters); \code{strat} a stratification variable for sampling with two levels, \code{"L"} and \code{"M"}, which are categorized by landscape metrics at each site; \code{count}, the number of moose at each site; \code{presence}, a factor that indicates whether at least one moose was observed at each site (\code{0} implies no moose; \code{1} implies at least one moose); and \code{geometry}, the NAD83/Alaska Albers (EPSG: 3338) projected coordinate of each site (these data are point-referenced because each observation occurs at point coordinates and are represented by a \code{POINT} geometry. The \code{moose_preds} data in \pkg{spmodel} contain spatial locations at which predictions of moose presence are desired (and is also point-referenced). \code{moose_preds} is also an \code{sf} object with measurements for \code{elev} and \code{strat} and the same projection system. Figure$~$\ref{fig-moose-data} shows the \code{presence} variable in \code{moose} as well as the spatial locations of both \code{moose} and \code{moose_preds}. Moose are most commonly present in the southwestern and eastern parts of the domain and least commonly present in the northwest (Figure$~$\ref{fig-moose-data}). Next we show how to use \pkg{spmodel} to study the effect of elevation and strata on moose presence while accounting for spatial covariance and to make predictions of moose presence at new locations.

\begin{figure}
\centering
\includegraphics[width = 0.70\linewidth]{figures/figure-1.png}
\caption{Moose presence in Alaska. Circles represent moose presence or absence (based on color) and triangles represent locations at which moose presence probability predictions are desired.}
\label{fig-moose-data}
\end{figure}

## Model Fitting

SPGLMs in \pkg{spmodel} are fit using the \code{spglm()} function. The \code{spglm()} function requires four arguments: `formula`, the relationship between the response and explanatory variables; `family`, the response distribution assumed for the repsonse variable; `data`, the data frame that contains the variables in `formula`, and `spcov_type`, the type of spatial covariance. These first three arguments are the three required arguments to \code{glm()} for nonspatial GLMs. So, the transition from \code{glm()} to \code{spglm()}
simply requires one additional argument: \code{spcov_type}. When \code{data} is not an \code{sf} object, \code{spglm()} also requires the \code{xcoord} and \code{ycoord} arguments, which indicate the columns in \code{data} that represent the x- and y-coordinates, respectively (it is assumed these coordinates are already projected).

We use \code{spglm()} to fit a spatial logistic regression model quantifying the effect of elevation and strata on moose presence:
```{r}
spbin <- spglm(
  formula = presence ~ elev + strat,
  family = binomial,
  data = moose,
  spcov_type = "exponential"
)
```

The \code{summary()} function returns a model summary that returns relevant information like the function call, deviance residuals, a coefficients table of fixed effects, the pseudo R-squared, spatial covariance parameter coefficient estimates, and the GLM dispersion parameter (fixed at one in logistic regression):
```{r}
summary(spbin)
```

Based on this model, there is some evidence that elevation is associated with higher probabilities of moose presence ($p$-value $\approx$ 0.087) but strong evidence that moose are more prevalent in the \code{"M"} strata than the \code{"L"} strata ($p$-value < 0.001). The fixed effects coefficients table from \code{summary()} is often of primary practical interest, but it is not easily usable when printed directly to the \proglang{R} console. The \code{tidy()} function tidies this table, turning it into a data frame (i.e., a tibble) with standard column names:
```{r}
tidy(spbin, conf.int = TRUE)
```

## Model Comparison

The strength of spatial covarinace in the data affects how beneficial a SPGLM is relative to a GLM. When the spatial covariance is strong, the SPGLM should notably outperform the GLM. When the spatial covariance is weak, the SPGLM and GLM should perform similarly. We can quantify the benefits of incorporating spatial covariance for a particular data set by comparing the fit of a SPGLM to a GLM. We can fit a GLM in \code{spmodel} by specifying \code{spcov_type = "none"}: 

```{r}
bin <- spglm(
  formula = presence ~ elev + strat,
  family = binomial,
  data = moose,
  spcov_type = "none"
)
```

While the \code{spglm()} approach evaluates the HGLMM likelhood with $\sigma^2_{de} = 0$ and $\sigma^2_{ie} \approx 0$ instead of just the GLM likelihood, the parameter estimates and their standard errors are the same:

```{r}
bin_glm <- glm(
  formula = presence ~ elev + strat,
  family = binomial,
  data = moose,
)
round(coef(bin), digits = 4)
round(coef(bin_glm), digits = 4)
round(sqrt(diag(vcov(bin))), digits = 4)
round(sqrt(diag(vcov(bin_glm))), digits = 4)
```

However, using \code{spglm()} instead of \code{glm()} ensures that \pkg{spmodel} helper functions are available and that each of the \code{spglm()} models uses the same likelihood:
```{r}
glance(spbin)
glance(bin)
```

The likelihood-based statistics AIC, AICc, BIC, and deviance are much lower for the SPGLM, indicating a better fit relative to the GLM. We may also perform a likelihood ratio test (LRT) between the two models, as the GLM is a special case of the SPGLM (i.e., is nested within the SPGLM):
```{r}
tidy(anova(spbin, bin))
```

The LRT test statistic is $\approx$ 31.5 with three degrees of freedom (the difference in covariance parameters), yielding a $p$-value $< 0.001$ that indicates preference for the full model (SPGLM) relative to the reduced model (GLM).

An alternative approach to model comparison is to use a cross-validation procedure [@james2013introduction]. The \code{loocv()} function performs leave-one-out cross validation, comparing the predicted mean (on the response scale) to the observed response variable for each hold-out observation, recomputing estimates of $\boldsymbol{\beta}$ each time. Then, statistics like bias, mean-squared-prediction error (MSPE), and the square root of MSPE (RMSPE) can be used to evaluate models:

```{r}
loocv(spbin)
loocv(bin)
```

Both models have negligible bias, but the SPGLM has much lower MSPE and RMSPE than the GLM, indicating the SPGLM predictions are far more efficient. Three separate metrics (likelihood-based statistics, likelihood-ratio test, and leave-one-out cross validation) prefer the SPGLM to the GLM.

We can compare two SPGLMs with different spatial covariance functions using likelihood-based statistics and leave-one-out cross validation, but we can't use the LRT because generally, the spatial covariance functions aren't nested:
```{r}
spbin2 <- update(spbin, spcov_type = "spherical")
glances(spbin, spbin2)
loocv(spbin)
loocv(spbin2)
```

The \code{"exponential"} spatial covariance (\code{spbin}) has a slightly lower deviance but slightly higher AIC, AICc, and BIC than 
the \code{"spherical"} spatial covariance (\code{spbin2}). Both spatial covariance functions nearly identical leave-one-out cross validation metrics. For practical purposes, these models fit quite similarly.

## Model Diagnostics

\code{spmodel} provides a suite of tools for model diagnostics. 

## Prediction

# Additional applications {#sec-applications2}

## Modeling moose counts in Alaska, USA

## Modeling harbor seal trends in Alaska, USA

## Modeling voter turnout in Texas, USA

## Modeling lake conductivity in Southwest, USA

# Discussion

\bibliography{references.bib}
