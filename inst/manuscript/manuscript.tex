\documentclass[
]{jss}

%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern}

\usepackage[utf8]{inputenc}

\author{
Michael Dumelle~\orcidlink{0000-0002-3393-5529}\\United States\\
Environmental Protection Agency \And Jay M. Ver
Hoef~\orcidlink{0000-0003-4302-6895}\\Alaska Fisheries\\
Science Center \And Matt
Higham~\orcidlink{0009-0006-4217-625X}\\St.~Lawrence University
}
\title{Spatial Generalized Linear Models in \proglang{R} Using
\pkg{spmodel}}

\Plainauthor{Michael Dumelle, Jay M. Ver Hoef, Matt Higham}
\Plaintitle{Spatial Generalized Linear Models in R Using spmodel}
\Shorttitle{Spatial Generalized Linear Models in \proglang{R} Using
\pkg{spmodel}}


\Abstract{
Non-Gaussian data are common in practice and include binary, count,
skewed, and proportion data types. Often, non-Gaussian data are modeled
using a generalized linear model (GLM). GLMs typically assume that
observations are independent of one another. This is an impractical
assumption for spatial data, as nearby observations tend to be more
similar than distant ones. The \pkg{spmodel} package in \proglang{R}
provides a suite of tools for fitting spatial generalized linear models
(SPGLMs) to non-Gaussian data and making spatial predictions (i.e.,
Kriging). SPGLMs for point-referenced (x- and y-coordinates) support are
fit using the \code{spglm()} function, while SPGLMs for areal (lattice,
polygon) support are fit using the \code{spgautor()} function. Both
\code{spglm()} and \code{spgautor()} maximize a novel Laplace likelihood
which marginalizes over the model's fixed effects and latent mean while
formally incorporating spatial covariance among observations. The inputs
and outputs of \code{spglm()} and \code{spgautor()} closely resemble the
\code{glm()} function from base \proglang{R}, easing the transition from
GLMs to SPGLMs. \pkg{spmodel} provides and builds upon several commonly
used helper functions for model building like \code{summary()},
\code{plot()}, and \code{fitted()}, among others. Spatial predictions of
the latent mean at unobserved locations are obtained using
\code{predict()} or \code{augment()}. \pkg{spmodel} accommodates myriad
advanced modeling features like geometric anisotrpoy, nonspatial random
effects, analysis of variance, and more. Throughout, we use
\pkg{spmodel} to fit SPGLMs to moose presence and counts in Alaska,
United States (US), skewed conductivity data in the Southwestern US,
harbor seal abundance trends in Alaska, US, and voter turnout rates in
Texas, US.
}

\Keywords{autoregressive model, geostatistical model, Poisson
regression, link function, logistic regression, overdispersion, spatial
covariance, spatial dependence}
\Plainkeywords{autoregressive model, geostatistical model, Poisson
regression, link function, logistic regression, overdispersion, spatial
covariance, spatial dependence}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
    Michael Dumelle\\
    United States\\
Environmental Protection Agency\\
    200 SW 35th St\\
Corvallis, OR, 97330\\
  E-mail: \email{Dumelle.Michael@epa.gov}\\
  
      }


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}




\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm, bbm}
\usepackage{lineno}
\usepackage{caption, subcaption}
\usepackage{autonum}
\usepackage{adjustbox}

\begin{document}



\linenumbers

\newpage

\section{Introduction}\label{sec-intro}

In practice, non-Gaussian data (e.g., binary, count, skewed, and
proportion data) are ubiquitous. Non-Gaussian data that belong to an
exponential family can be naturally modeled using a generalized linear
model (GLM) regression framework
\citep{nelder1972generalized, mccullagh1989generalized}. In a GLM, an
\(n \times 1\) response variable \(\mathbf{y}\) belongs to a statistical
distribution (e.g., binomial, Poisson) with some mean and variance.
Often, the analysis goal is to study the impact of a linear function of
several explanatory variables on the mean of \(\mathbf{y}\) through a
GLM. In this context, the latent (i.e., unobserved) mean of
\(\mathbf{y}\), \(\boldsymbol{\mu}\), is linked to these explanatory
variables via a link function: \begin{align}\label{eq-glm}
f(\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta}) \equiv \mathbf{w} = \mathbf{X} \boldsymbol{\beta},
\end{align} where for a sample size \(n\), \(f(\cdot)\) is a link
function that connects \(\boldsymbol{\mu}\) to \(\mathbf{w}\),
\(\mathbf{X}\) is the \(n \times p\) design matrix of explanatory
variables, and \(\boldsymbol{\beta}\) is the \(p \times 1\) vector of
fixed effects. While the mean is typically constrained in some way
(e.g., if a probability, between zero and one), the link function
generally makes \(\mathbf{w}\) unconstrained. Common link functions
include the log odds (i.e., logit) link for binary and proportion data
and the log link for count and skewed data. Equation\(~\)\ref{eq-glm}
can also be written in terms of the inverse link function,
\(f^{-1}(\cdot)\): \begin{align}\label{eq-glm2}
\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta} \equiv f^{-1}(\mathbf{w}) = f^{-1}(\mathbf{X} \boldsymbol{\beta}).
\end{align}

The GLM fixed effects (\(\boldsymbol{\beta}\)) are typically estimated
via maximum likelihood \citep{chambers1992S}. It is often convenient to
compute the maximum likelihood estimates using the iteratively
reweighted least squares (IRWLS) algorithm \citep{wood2017generalized},
which is an approach used by the \code{glm()} function in the
\proglang{R} programming language \citep{rcore2024}. GLMs add an
additional layer of complexity compared to linear regression models, as
the left-hand side of Equation\(~\)\ref{eq-glm} is a function of the
mean of \(\mathbf{y}\) rather than \(\mathbf{y}\) itself (as in linear
regression models).

The standard GLM assumes the elements of \(\mathbf{y}\) are independent.
This independence assumption is typically impractical for spatial data.
For spatial data, nearby observations tend to be more similar than
distant observations \citep{tobler1970computer}, which leads to positive
spatial covariance among observations. The consequences of ignoring
spatial covariance in statistical models for spatial data can be severe
and include imprecise parameter estimates as well as misleading standard
errors that inflate Type-I error rates and decrease power
\citep{zimmerman2024spatial}.

An approach for handling spatial data using a GLM is to assume the
elements of \(\mathbf{w}\) exhibit both spatial and nonspatial
variability. This is achieved by adding to Equation\(~\)\ref{eq-glm} two
random effects, \(\boldsymbol{\tau}\) and \(\boldsymbol{\epsilon}\). The
random effect \(\boldsymbol{\tau}\) is an \(n \times 1\) column vector
of spatially dependent random errors. We assume that
\(\text{E}(\boldsymbol{\tau}) = \boldsymbol{0}\) and
\(\text{Cov}(\boldsymbol{\tau}) = \sigma^2_\tau \mathbf{R}\), where
\(\text{E}(\cdot)\) and \(\text{Cov}(\cdot)\) denote expectation and
covariance, respectively. The variance parameter \(\sigma^2_\tau\)
controls the magnitude of spatial covariance and is often called a
partial sill. The matrix \(\mathbf{R}\) is an \(n \times n\) spatial
correlation matrix that depends on a range parameter controlling the
distance-decay rate of the spatial correlation. One example of a spatial
covariance matrix is the ``exponential,'' which is given by
\begin{align}\label{eq-spcov-exp}
  \text{Cov}(\boldsymbol{\tau}) = \sigma^2_{\tau} \mathbf{R}_{exp} =  \sigma^2_{\tau} \exp(-\mathbf{H}/\phi),
\end{align} where \(\mathbf{H}\) is a matrix of pairwise distances among
the elements of \(\mathbf{y}\) and \(\phi\) is the range parameter. From
Equation\(~\)\ref{eq-spcov-exp}, as the distance between two elements of
\(\mathbf{y}\) increases, the spatial covariance decreases, which
reflects intuition. Moreover, as the range parameter, \(\phi\),
increases, the strength of spatial dependence increases
(Figure\(~\)\ref{fig-range}). The random effect
\(\boldsymbol{\epsilon}\) is an \(n \times 1\) column vector of
independent random errors. We assume that
\(\text{E}(\boldsymbol{\epsilon}) = \boldsymbol{0}\) and
\(\text{Cov}(\boldsymbol{\tau}) = \sigma^2_\epsilon \mathbf{I}\), where
\(\mathbf{I}\) is an \(n \times n\) identity matrix. The variance
parameter \(\sigma^2_\epsilon\) controls the magnitude of nonspatial
variability (i.e., fine-scale variation) and is often called a nugget.
Often in spatial statistics, quantities are explicitly referenced with
respect to \(\mathbf{s}\), a vector of spatial coordinates indexing the
observation \citep{cressie1993statistics}. For example, \(\mathbf{y}\)
and \(\mathbf{X}\) may instead be written \(\mathbf{y}(\mathbf{s})\) and
\(\mathbf{X}(\mathbf{s})\), respectively. We acknowledge the utility of
this nomenclature but drop the explicit dependence on \(\mathbf{s}\) for
simplicity of notation moving forward.

\begin{figure}
\centering
\includegraphics[width = 0.7\linewidth]{figures/figure-01.png}
\caption{An exponential spatial correlation function with varying range parameters.}
\label{fig-range}
\end{figure}

Through inclusion of \(\boldsymbol{\tau}\) and
\(\boldsymbol{\epsilon}\), the spatial GLM (SPGLM) can be written as
\begin{align}\label{eq-spglm}
f(\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{\epsilon}) \equiv \mathbf{w} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon}.
\end{align} Assuming independence among \(\boldsymbol{\tau}\) and
\(\boldsymbol{\epsilon}\), it follows that \begin{align}\label{eq-spcov}
 \text{Cov}(\boldsymbol{\tau} + \boldsymbol{\epsilon}) = \text{Cov}(\boldsymbol{\tau}) + \text{Cov}(\boldsymbol{\epsilon}) = \sigma^2_{\tau}\mathbf{R} + \sigma^2_{\epsilon} \mathbf{I}.
\end{align} Henceforth, we refer to \(\sigma^2_{\tau}\) as
\(\sigma^2_{de}\) (for spatially dependent error variance) and
\(\sigma^2_{\epsilon}\) as \(\sigma^2_{ie}\) (for independent error
variance). The parameters \(\sigma^2_{de}\), \(\sigma^2_{ie}\), and
\(\phi\), in addition to any other parameters in \(\mathbf{R}\), compose
\(\boldsymbol{\theta}\), the covariance parameter vector.

Fitting and using SPGLMs is challenging both conceptually and
computationally \citep{bolker2009generalized}. Recently, however, there
have been numerous, significant advances in \proglang{R} software that
have made these models more accessible to practitioners. The \pkg{brms}
\citep{burkner2017brms}, \pkg{carBayes} \citep{lee2013carbayes},
\pkg{ngspatial} \citep{hughes2020ngspatial}, \pkg{R-INLA}
\citep{lindgren2015bayesian}, \pkg{inlabru} \citep{bachl2019inlabru},
\pkg{spBayes} \citep{finley2007spbayes}, \pkg{spOccupancy}
\citep{doser2022spoccupancy}, \pkg{spAbundance}
\citep{doser2024spabundance}, and \pkg{spNNGP} \citep{finley2002spnngp}
packages take a Bayesian approach, either directly sampling from
posterior distributions of parameters (e.g., using MCMC) or
approximating them. A benefit of Bayesian approaches is that prior
information can be incorporated and uncertainty quantification of
parameter estimates is straightforward. However, Bayesian approaches,
especially those using MCMC, can be computationally expensive. In order
to reduce computation time, many of these packages (e.g., \pkg{R-INLA})
work with the precision matrix instead of the covariance matrix so that
computationally expensive matrix inversion is not required. Working with
precision matrices, however, can be more restrictive and less intuitive
than working with covariance matrices. The \pkg{FRK}
\citep{sainsbury2024modeling}, \pkg{glmmTMB} \citep{brooks2017glmmtmb},
\pkg{hglm} \citep{ronnegard2010hglm}, \pkg{mgcv}
\citep{wood2017generalized}, and \pkg{spaMM} \citep{rousset2014spamm}
packages directly use Laplace, quasi-likelihood, or reduced-rank
approaches to estimate parameters. These direct approaches tend to be
computationally efficient, as they don't rely on MCMC sampling. In
contrast to the Bayesian approach, a drawback of these direct approaches
is that prior information cannot be formally incorporated and covariance
parameter uncertainty is more challenging to quantify. The \pkg{sdmTMB}
\citep{anderson2024sdmtmb} package combines elements of \pkg{R-INLA},
\pkg{glmmTMB}, and Gaussian Markov random fields to fit a wide variety
of SPGLMs, while \pkg{tinyVAST} \citep{thorson2025tinyVAST} extends some
of these models to multivariate or (dynamic) structural equation models.

Building from \citet{evangelou2011estimation} and
\citet{bonat2016practical}, \citet{ver2024marginal} proposed a novel
approach for fitting SPGLMs that leverages the Laplace approximation
while marginalizing over both the latent \(\mathbf{w}\) and the fixed
effects (\(\boldsymbol{\beta}\)). This approach performed well in a
variety of simulation settings, generally having appropriate confidence
interval coverage for the fixed effects and prediction interval coverage
for \(\mathbf{w}\) at new locations. It also performed similarly to the
Bayesian SPGLM approach in \pkg{spBayes} and the automatic
differentiation SPGLM approach in \pkg{glmmTMB} but was much faster. At
small sample sizes, the approach outperformed the approximate Bayesian
SPGLM approach in \pkg{R-INLA} and had similar computational times. For
moderate sample sizes, it performed similarly to \pkg{R-INLA}, though
\pkg{R-INLA} was faster. The novel Laplace approach is particularly
attractive for two reasons. First, it is general enough that it can be
applied to any covariance structure (not just spatial). Second, after
estimating the covariance parameters, analytical solutions exist for the
fixed effects (and their standard errors) as well as predictions of the
latent \(\mathbf{w}\) at new locations (and their standard errors).

The \pkg{spmodel} \proglang{R} package \citep{dumelle2023spmodel}
recently released a full set of modeling tools for SPGLMs fit using the
novel Laplace approach described by \citet{ver2024marginal}. These
modeling tools are approachable and mirror the familiar \code{glm()}
syntax from base-\proglang{R}, making the transition from GLMs to SPGLMs
relatively seamless. The \code{spglm()} function fits SPGLMs for
point-referenced support (e.g., x- and y-coordinates representing point
locations in a field; these models are sometimes called
``geostatistical'' models), while the \code{spgautor()} function fits
SPGLMs for areal support (e.g., polygon boundaries representing
geographic subsets of a region; these models are sometimes called
``autoregressive'' models). For both point-referenced and areal
supports, \pkg{spmodel} supports the binomial distribution for binary
data, Poisson and negative binomial distributions for count data, Gamma
and inverse Gaussian distributions for skewed data, and the beta
distribution for proportion data. There are 20 different spatial
covariance structures available including the exponential, Gaussian, and
spherical for point-referenced support (Figure\(~\)\ref{fig-type}) and
the conditional autoregressive, and simultaneous autoregressive
structures for areal support \pkg{spmodel} provides tools for commonly
used model summaries, visualizations, and diagnostics (e.g., fitted
values) using standard \proglang{R} helper functions like
\code{summary()}, \code{plot()}, and \code{fitted()}, among others.
\pkg{spmodel} also provides tools to predict \(\mathbf{w}\) at new
locations and quantify uncertainty in those prediction using
\code{predict()} and \code{augment()}. This core functionality, combined
with several advanced features we describe throughout the manuscript,
enables \pkg{spmodel} to introduce novel, important SPGLM modeling tools
previously missing from the existing \proglang{R} ecosystem.

\begin{figure}
\centering
\includegraphics[width = 0.7\linewidth]{figures/figure-02.png}
\caption{Exponential, Gaussian, and spherical spatial correlation functions all with range parameters equal to 0.5.}
\label{fig-type}
\end{figure}

Of the existing \proglang{R} packages for SPGLMs, \pkg{spmodel} (version
0.11.1) is arguably most similar to \pkg{sdmTMB} (version 0.7.4) in
terms of scope and feel. Both packages use similar syntax as
\code{glm()}, accommodate flexible \code{formula} arguments (e.g.,
offsets, splines), handle spatial covariance that decays at different
rates in different directions (i.e., geometric anisotropy), incorporate
nonspatial random effects, support other \proglang{R} packages for
modeling like \pkg{broom} \citep{robinson2021broom, kuhn2022tidy},
\pkg{emmeans} \citep{lenth2024emmeans}, and \pkg{car}
\citep{fox2019car}, and have tools for model summaries, prediction, and
simulating data. There are some notable differences between the two
packages, however. \pkg{sdmTMB} supports several additional GLM
distributions like the Tweedie, supports Hurdle models, and can
incorporate prior information through Bayesian applications.
\pkg{sdmTMB} also provides tools for working with temporal data and
spatiotemporal data and provides enhanced visualizations of the model's
marginal effects. \pkg{sdmTMB} does require a preprocessing step of
constructing a mesh prior to model fitting (using the stochastic partial
differential equation approach), and the density of the mesh can affect
model results and computational complexity. On the other hand,
\pkg{spmodel} does not require the construction of a mesh prior to model
fitting. \pkg{spmodel} supports 20 different spatial covariances and
models them directly, rather than using a precision matrix approximation
to the MatÃ©rn spatial covariance as in \pkg{sdmTMB}. \pkg{spmodel} can
model data directly using neighborhood distance and autoregressive
models, rather than relying on the polygon centroid (as in
\pkg{sdmTMB}), which may not be within the polygon's boundaries.
\pkg{spmodel} provides experimental design tools (e.g., analysis of
variance, contrasts), supports \pkg{sf} objects in modeling and
prediction functions \citep{pebesma2018sf}, has several specialized
model diagnostics like leverage values and Cook's distances, and has
analytic solutions for fixed effect and prediction standard errors.
Other similarities and differences do exist between \pkg{sdmTMB} and
\pkg{spmodel}, and both packages continue to evolve. Overall, we believe
that these packages are complementary and enhance the suite of SPGLM
tools accessible to practitioners.

The rest of this article is organized as follows. In
Section\(~\)\ref{sec-spglm}, we provide some background for the SPGLM
fitting and prediction routines in \pkg{spmodel}. In
Section\(~\)\ref{sec-applications}, we provide an overview of core SPGLM
functionality in \pkg{spmodel} by modeling moose presence in Alaska,
United States (US). In Section\(~\)\ref{sec-applications2}, we model
moose counts in Alaska, US; skewed lake conductivity in the Southwestern
US; harbor seal abundance trend behavior in Alaska, US; and voter
turnout rates in Texas, US. And in Section\(~\)\ref{sec-discussion}, we
end with a discussion synthesizing \pkg{spmodel}'s contributions to
SPGLMs in \proglang{R}.

\section{The spatial generalized linear model and
marginalization}\label{sec-spglm}

The novel Laplace approach implemented in \pkg{spmodel} formally
maximizes a hierarchical GLM likelihood
\citep{lee1996hierarchical, wood2017generalized}, making
likelihood-based statistics for model comparison like AIC
\citep{akaike1974new}, AICc \citep{hoeting2006model}, BIC
\citep{schwarz1978estimating}, deviance
\citep{mccullagh1989generalized}, and likelihood ratio tests available.
These types of statistics are not available for quasi-likelihood
\citep{wedderburn1974quasi, breslow1993approximate} or pseudo-likelihood
approaches \citep{wolfinger1993generalized}, which only specify the
first two moments of a distribution. Next, we describe a brief overview
of the approach and how it can be used for several primary data analysis
tasks \citep{tredennick2021practical} like model comparison, parameter
estimation, inference, model diagnostics, and prediction.

\subsection{Formulating the hierarchical
likelihood}\label{formulating-the-hierarchical-likelihood}

We can write the SPGLM likelihood hierarchically as
\begin{align}\label{eq-marginal}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} \int_{\boldsymbol{\beta}} [\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\beta}, \boldsymbol{\theta}] d\boldsymbol{\beta} d\mathbf{w},
\end{align} where \([\mathbf{y} | f^{-1}(\mathbf{w}), \varphi]\) is the
density for the appropriate response distribution of \(\mathbf{y}\)
(e.g., binomial, Poisson) given the latent \(\mathbf{w}\) and dispersion
parameter (\(\varphi\)), and
\([\mathbf{w} | \mathbf{X}, \boldsymbol{\beta}, \boldsymbol{\theta}]\)
is the multivariate Gaussian density for \(\mathbf{w}\) given the
explanatory variables (\(\mathbf{X}\)), fixed effects
(\(\boldsymbol{\beta}\)), and spatial covariance parameters
(\(\boldsymbol{\theta}\)). The elements of
\([\mathbf{y} | f^{-1}(\mathbf{w}), \varphi]\) are conditionally
independent (given \(\mathbf{w}\)), but the elements of
\([\mathbf{w} | \mathbf{X}, \boldsymbol{\beta}, \boldsymbol{\theta}]\)
share spatial covariance. Following \citet{harville1974bayesian}, we can
integrate \(\boldsymbol{\beta}\) out of Equation\(~\)\ref{eq-marginal},
which yields \begin{align}\label{eq-marginal2}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} [\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] d\mathbf{w},
\end{align} where \([\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}]\) is
the restricted (i.e., residual) multivariate Gaussian density
\citep{patterson1971recovery} for \(\mathbf{w}\) given the explanatory
variables and covariance parameters. The restricted multivariate
Gaussian density is given by \begin{align}\label{eq-reml-def}
[\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] = \frac{\exp(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\tilde{\boldsymbol{\beta}}) \boldsymbol{\Sigma}^{-1} (\mathbf{y} - \mathbf{X}\tilde{\boldsymbol{\beta}})^\top)}{(2 \pi)^{(n - p)/2} |\boldsymbol{\Sigma}|^{1/2}|\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X}|^{1/2}},
\end{align} where
\(\tilde{\boldsymbol{\beta}} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{w}\),
\(\boldsymbol{\Sigma}\) denotes the covariance matrix (of
\(\mathbf{w}\)), and \(|\cdot|\) denotes the determinant.
Equation\(~\)\ref{eq-marginal2} can synonymously be written after
profiling the overall variance out of \(\boldsymbol{\Sigma}\), which
reduces the dimension of \(\boldsymbol{\theta}\) by one for optimization
\citep{wolfinger1994computing}. Next, let
\begin{align}\label{eq-marginal03}
  \ell_\mathbf{w} = \log([\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}])
\end{align} and rewrite Equation\(~\)\ref{eq-marginal2} as
\begin{align}\label{eq-marginal3}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} \exp(\ell_\mathbf{w}) d\mathbf{w}.
\end{align} A second-order Taylor series expansion of
\(\ell_\mathbf{w}\) around a point \(\mathbf{w}^{*}\) yields
\begin{align}\label{eq-marginal4}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx \int_{\mathbf{w}} \exp(\ell_{\mathbf{w}^{*}} + \mathbf{g}^\top(\mathbf{w} - \mathbf{w}^{*}) + \frac{1}{2}(\mathbf{w} - \mathbf{w}^{*})^\top \mathbf{G} (\mathbf{w} - \mathbf{w}^{*}))d\mathbf{w},
\end{align} where \(\mathbf{g}\) and \(\mathbf{G}\) are the gradient and
Hessian, respectively, of \(\ell_\mathbf{w}\) with respect to
\(\mathbf{w}\). If \(\mathbf{w}^{*}\) is a value for which
\(\mathbf{g} = \mathbf{0}\), \begin{align}\label{eq-marginal5}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx \exp(\ell_{\mathbf{w}^{*}}) \int_{\mathbf{w}} \exp(-\frac{1}{2}(\mathbf{w} - \mathbf{w}^{*})^\top (-\mathbf{G}) (\mathbf{w} - \mathbf{w}^{*}))d\mathbf{w}.
\end{align} The integral in Equation\(~\)\ref{eq-marginal5} can be
solved by leveraging properties of the normalizing constant of a
multivariate Gaussian distribution. Thus, rewriting
\(\exp(\ell_{\mathbf{w}^{*}})\) yields \begin{align}\label{eq-marginal6}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx [\mathbf{y} | f^{-1}(\mathbf{w}^{*}), \varphi] [\mathbf{w}^{*} | \mathbf{X}, \boldsymbol{\theta}] (2 \pi)^{n/2}|-\mathbf{G}_{\mathbf{w}^{*}}|^{-1/2},
\end{align} which can be directly evaluated. This result suggests a
doubly iterative optimization over 1) \(\boldsymbol{\theta}\) and
\(\varphi\) and 2) the latent \(\mathbf{w}\) (to find each set of
\(\mathbf{w}^{*}\)), which ultimately yields the marginal restricted
maximum likelihood estimators \(\hat{\varphi}\) and
\(\hat{\boldsymbol{\theta}}\) and their respective values of
\(\mathbf{w}^{*}\), which we call \(\hat{\mathbf{w}}\).
\citet{ver2024marginal} provide further details, which includes explicit
forms of \(\mathbf{g}\) and \(\mathbf{G}\) for various response
distributions.

\subsection{Estimating fixed effects}\label{sec:fixed}

We can estimate the fixed effects using generalized least squares (GLS)
principles, a common practice for linear models estimated using
restricted maximum likelihood methods. Had we observed \(\mathbf{w}\), a
GLS estimator for \(\boldsymbol{\beta}\) is given by
\begin{align}\label{eq-gls1}
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{w} = \mathbf{B}\mathbf{w},
\end{align} where
\(\mathbf{B} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\).
While we do not \(\mathbf{w}\), we do estimate it via
\(\hat{\mathbf{w}}\), and thus it is reasonable to define
\(\hat{\boldsymbol{\beta}} = \mathbf{B}\hat{\mathbf{w}}\). To derive
properties of \(\hat{\boldsymbol{\beta}}\) like expectation and
variance, we must derive these properties for \(\hat{\mathbf{w}}\) by
conditioning on \(\mathbf{w}\) as if it were observed and leveraging the
laws of total expectation and variance. Based on asymptotic properties
of (restricted) maximum likelihood estimators
\citep{cressie1993asymptotic}, we may assume that given \(\mathbf{w}\),
\(\hat{\mathbf{w}}\) has mean \(\mathbf{w}\) and variance equal to
\(-\mathbf{H}^{-1}\), the negative inverse Hessian (i.e., the inverse
observed information matrix). Thus it follows that
\(\text{E}(\hat{\mathbf{w}})\) is given by \begin{align}
  \text{E}(\hat{\mathbf{w}}) = \text{E}(\text{E}(\hat{\mathbf{w}} | \mathbf{w}))
   = \text{E}(\mathbf{w}) 
   = \mathbf{X}\boldsymbol{\beta}
\end{align} and \(\text{Var}(\hat{\mathbf{w}})\) is given by
\begin{align}
  \text{Var}(\hat{\mathbf{w}}) & = \text{E}(\text{Var}(\hat{\mathbf{w}} | \mathbf{w})) + \text{Var}(\text{E}(\hat{\mathbf{w}} | \mathbf{w})) \\
  & = \text{E}(-\mathbf{H}^{-1}) + \text{Var}(\mathbf{w})\\
  & = -\mathbf{H}^{-1} + \boldsymbol{\Sigma}
\end{align} Putting this all together, it follows that
\(\hat{\boldsymbol{\beta}}\) is unbiased for \(\boldsymbol{\beta}\):
\begin{align}
  \text{E}(\hat{\boldsymbol{\beta}})  = \text{E}(\mathbf{B}\hat{\mathbf{w}}) 
  = \mathbf{B}\text{E}(\hat{\mathbf{w}}) =  (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})\boldsymbol{\beta} = \boldsymbol{\beta}.
\end{align} Moreover, it follows that \begin{align}
  \text{Var}(\hat{\boldsymbol{\beta}}) & = \text{Var}(\mathbf{B}\hat{\mathbf{w}}) \\
  & = \mathbf{B} \text{Var}(\hat{\mathbf{w}}) \mathbf{B}^\top\\
  & = \mathbf{B} (-\mathbf{H}^{-1} + \boldsymbol{\Sigma}) \mathbf{B}^\top \\
  & = \mathbf{B}(-\mathbf{H})^{-1}\mathbf{B}^\top + \mathbf{B}\boldsymbol{\Sigma}\mathbf{B}^\top \\
  & = \mathbf{B}(-\mathbf{H})^{-1}\mathbf{B}^\top + (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}.
\end{align} In practice, \(\text{Var}(\hat{\boldsymbol{\beta}})\) is
estimated by evaluating \(\boldsymbol{\Sigma}\) at
\(\hat{\boldsymbol{\theta}}\), the estimated covariance parameter
vector.

These results are important because they justify analytic (i.e.,
closed-form) solutions for \(\hat{\boldsymbol{\beta}}\) and its
associated variance. Analytic solutions are useful because they bypass
the need for sampling-based strategies to evaluate the mean and variance
of \(\hat{\boldsymbol{\beta}}\), a common technique for other approaches
to SPGLMs like Bayesian MCMC that can be computationally intensive.

\subsection{Inspecting model diagnostics}\label{sec:diagnostics}

Inspecting model diagnostics is an important step of the modeling
process that can yield valuable insights into model behavior and unusual
observations. \citet{montgomery2021introduction} contextualize three
components of unusual observations: outliers, leverage, and influence.
An observation is an outlier if it has an extreme response value
relative to expectation. The response GLM residuals simply compare the
observation to its fitted latent mean: \begin{align}
  \mathbf{r}_{r} = \mathbf{y} - f^{-1}(\hat{\mathbf{w}})
\end{align} Because observations often have a unique support in a GLM
(e.g., only two possible response values for binary data) and the
variance of an observation generally depends on its mean, response
residuals lack some utility. Deviance residuals are a function of
response residuals that are appropriately scaled to behave more like
response residuals in a standard linear model. Deviance residuals are
given by \begin{align}
  \mathbf{r}_{d} = sign(\mathbf{r}_{r})\sqrt{\mathbf{d}},
\end{align} where \(\mathbf{d}\) is a vector of individual deviances.
The sum of the squared deviance residuals equals the sum of the elements
of \(\mathbf{d}\), known as the deviance of the model fit. The deviance
of the model fit quantifies twice the difference in log likelihoods
between the a saturated model that fits every observation perfectly
(i.e., \(\text{y}_i = f^{-1}(\hat{\text{w}}_i)\) for all \(i\)) and the
fitted model \citep{myers2012generalized}. Deviance is often used as a
fit statistic; lower values of deviance imply a better model fit
(compared to the observed data). Pearson and standardized residuals are
other types of GLM residuals that involve a scaling of the response
residuals. The Pearson residuals scale \(\mathbf{r}_{r}\) by the square
root of \(\mathbf{V}\), a diagonal matrix with \(i\)th diagonal element
equal to the variance of the response distribution evaluated at
\(f^{-1}(\hat{\text{w}}_i)\) \citep{faraway2016extending};
\(\mathbf{V}\) is sometimes called the GLM weight matrix. The
standardized residuals scale the deviance residuals by
\(\frac{1}{\sqrt{(1 - \mathbf{L}_{ii})}}\), where \(\mathbf{L}_{ii}\) is
the \(i\)th diagonal element of the leverage matrix, which we discuss
next.

An observation has high leverage if its combination of explanatory
variables is far away from other observations. In a linear model, the
leverage (i.e., hat) values are the diagonal of the leverage (i.e.,
projection, hat) matrix,
\(\mathbf{L} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\).
In a GLM, the leverage matrix is given by \begin{align}
  \mathbf{L} = \mathbf{V}^{1/2} \mathbf{X} (\mathbf{X}^\top \mathbf{V} \mathbf{X}) \mathbf{X}^\top \mathbf{V}^{1/2}.
\end{align} The larger the value of \(\mathbf{L}_{ii}\), the more severe
the leverage from the \(i\)th observation.

An observation is influential if it has a sizable impact on model fit.
Influence is measured using Cook's distance
\citep{cook1979influential, cook1982residuals}, which is given for a GLM
by \begin{align}
  \mathbf{c} = \frac{\mathbf{r}^2_{s}}{\text{tr}(\mathbf{L})} \frac{diag(\mathbf{L})}{(\mathbf{1} - diag(\mathbf{L}))},
\end{align} where \(\mathbf{r}^2_{s}\) are the standardized residuals
and \(diag(\mathbf{L})\) indicates the diagonal elements of the leverage
matrix. The larger the value of \(\text{c}_i\), the more severe the
influence from the \(i\)th observation.
\citet{montgomery2021introduction} provide guidance for interpreting
these types of statistics, including cutoffs to consider when
identifying extreme residual, leverage, or influence values.

In a linear model, the \(R^2\) (R-squared) statistic quantifies the
proportion of variability in the data captured by the explanatory
variables. It is calculated as one minus the ratio of the error sum of
squares to the total sum of squares \citep{rencher2008linear}. In a GLM,
there are many ways to define a statistic that emulates the
aforementioned meaning of \(R^2\) from the linear model
\citep{smith2013comparison}. This statistic is called a pseudo R-squared
(\(PR^2\)). One \(PR^2\) for GLMs simply replaces the sums of squares
ratio from the linear model with the deviance ratio: \begin{align}
  PR^2 = 1 - \frac{deviance_{error}}{deviance_{total}},
\end{align} where \(deviance_{error}\) is the deviance of the fitted
model (sometimes called the error or residual deviance) and
\(deviance_{total}\) is the deviance of the intercept-only model
(sometimes called the total or null deviance). In practice,
\(deviance_{total}\) is derived by computing \(\hat{\mathbf{w}}\) when
\(\mathbf{X} \equiv \mathbf{1}\) (a column of all ones), given
\(\hat{\boldsymbol{\theta}}\) and \(\hat{\varphi}\) from the fitted
model. Like \(R^2\), \(PR^2\) can be adjusted to account for the numbers
of parameters estimated in a model. Because the \(deviance_{total}\)
denominator changes across fitted models (as the values of
\(\hat{\boldsymbol{\theta}}\) and \(\hat{\varphi}\) change), this
statistic should not be used as a model comparison tool. Rather, it
should be used as an informative diagnostic tool that is unique to each
model fit and describes how much variability from that model is
attributable to the explanatory variables.

\subsection{Predicting at new
locations}\label{predicting-at-new-locations}

We may also predict values of the latent mean (on the link scale) at new
locations by leveraging the spatial covariance between observed
locations and new locations (spatial prediction is also called Kriging;
see \citet{cressie1990origins}). Like in Section\(~\)\ref{sec:fixed},
suppose first that we observed \(\mathbf{w}\) and we want to make
predictions at \(\mathbf{u}\), a vector of latent means at the new
locations that follows the same SPGLM from Equation\(~\)\ref{eq-spglm}
and has design matrix, \(\mathbf{X}_{\mathbf{u}}\). The vector
\((\mathbf{w}, \mathbf{u})^\top\) has expectation
\((\mathbf{X}\boldsymbol{\beta}, \mathbf{X}_\mathbf{u}\boldsymbol{\beta})^\top\)
and covariance matrix
\(\begin{bmatrix} \boldsymbol{\Sigma} & \boldsymbol{\Sigma}_{\mathbf{w}\mathbf{u}} \\ \boldsymbol{\Sigma}_{\mathbf{u}\mathbf{w}} & \boldsymbol{\Sigma}_{\mathbf{u}\mathbf{u}} \end{bmatrix}\),
where \(\boldsymbol{\Sigma} = \text{Var}(\mathbf{w}, \mathbf{w})\),
\(\boldsymbol{\Sigma}_{\mathbf{w} \mathbf{u}} = \text{Var}(\mathbf{w}, \mathbf{u})\),
\(\boldsymbol{\Sigma}_{\mathbf{u} \mathbf{w}} = \boldsymbol{\Sigma}_{\mathbf{w} \mathbf{u}}^\top\)
and
\(\boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{u}} = \text{Var}(\mathbf{u}, \mathbf{u})\).
Thus we may derive the conditional distribution of
\(\mathbf{u}|\mathbf{w}\), which has the following properties:
\begin{align}
  \text{E}(\mathbf{u} | \mathbf{w}) & = \mathbf{X}_{\mathbf{u}} \boldsymbol{\beta} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\mathbf{w} - \mathbf{X}\boldsymbol{\beta}) \\
  \text{Var}(\mathbf{u} | \mathbf{w}) & = \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}_{\mathbf{w}, \mathbf{u}}
\end{align} Recall, however, that we do not actually observe
\(\mathbf{w}\) and instead compute \(\hat{\mathbf{w}}\); so, to predict
\(\mathbf{u}\) and quantify its uncertainty, we must again leverage the
laws of total expectation and variance. \citet{ver2024marginal} show
that \(\hat{\mathbf{u}}\) and its associated variance are given by:
\begin{align}
  \hat{\mathbf{u}} & = \mathbf{X}_{\mathbf{u}} \hat{\boldsymbol{\beta}} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\hat{\mathbf{w}} - \mathbf{X}\hat{\boldsymbol{\beta}}) \\
  \text{Var}(\hat{\mathbf{u}}) & = \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}_{\mathbf{w}, \mathbf{u}} + \mathbf{K}(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{K}^\top + \boldsymbol{\Lambda}(-\mathbf{H})^{-1}\boldsymbol{\Lambda}^\top,
\end{align} where
\(\mathbf{K} = \mathbf{X}_{\mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \mathbf{X}\)
and
\(\boldsymbol{\Lambda} = \mathbf{X}_{\mathbf{u}}\mathbf{B} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\mathbf{1} - \mathbf{X}\mathbf{B})\)
for a vector of ones, \(\mathbf{1}\). As with
\(\hat{\boldsymbol{\beta}}\), these covariance matrices are evaluated at
\(\hat{\boldsymbol{\theta}}\) in practice.

\section{Modeling moose presence in Alaska, USA}\label{sec-applications}

The \code{moose} data in \pkg{spmodel} contain information on moose
(Alces Alces) presence in the Togiak region of Alaska, USA. \code{moose}
is an \code{sf} object, a special data frame that is supplemented with
spatial information using the \pkg{sf} package in \proglang{R}
\citep{pebesma2018sf}. After loading \pkg{spmodel}, the first few rows
of \code{moose} look like:

\begin{CodeChunk}
\begin{CodeInput}
R> library("spmodel")
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> head(moose)
\end{CodeInput}
\begin{CodeOutput}
Simple feature collection with 6 features and 4 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 281896.4 ymin: 1518398 xmax: 311325.3 ymax: 1541016
Projected CRS: NAD83 / Alaska Albers
# A tibble: 6 x 5
   elev strat count presence           geometry
  <dbl> <chr> <dbl> <fct>           <POINT [m]>
1  469. L         0 0        (293542.6 1541016)
2  362. L         0 0        (298313.1 1533972)
3  173. M         0 0        (281896.4 1532516)
4  280. L         0 0        (298651.3 1530264)
5  620. L         0 0        (311325.3 1527705)
6  164. M         0 0        (291421.5 1518398)
\end{CodeOutput}
\end{CodeChunk}

There are five columns: \code{elev}, the numeric site elevation
(meters); \code{strat} a stratification variable for sampling with two
levels, \code{"L"} and \code{"M"}, which are categorized by landscape
metrics at each site; \code{count}, the number of moose at each site;
\code{presence}, a factor that indicates whether at least one moose was
observed at each site (\code{0} implies no moose; \code{1} implies at
least one moose); and \code{geometry}, the NAD83/Alaska Albers (EPSG:
3338) projected coordinate of each site. These data are point-referenced
because each observation occurs at point coordinates and are represented
by a \code{POINT} geometry. Moose are most prevalent in the southwestern
and eastern parts of the Togiak region
(Figure\(~\)\ref{fig-moose-data}).

\begin{figure}
\centering
\includegraphics[width = 0.70\linewidth]{figures/figure-1.png}
\caption{Moose presence in Alaska. Circles represent moose presence or absence (based on color) and triangles represent locations at which moose presence probability predictions are desired.}
\label{fig-moose-data}
\end{figure}

The \code{moose_preds} data in \pkg{spmodel} is an \code{sf} object with
point locations at which moose presence predictions are desired. Like
\code{moose}, \code{moose_preds} contains \code{elev} and \code{strat}
for each site:

\begin{CodeChunk}
\begin{CodeInput}
R> head(moose_preds)
\end{CodeInput}
\begin{CodeOutput}
Simple feature collection with 6 features and 2 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 291839.8 ymin: 1436192 xmax: 401239.6 ymax: 1512103
Projected CRS: NAD83 / Alaska Albers
# A tibble: 6 x 3
   elev strat           geometry
  <dbl> <chr>        <POINT [m]>
1  143. L     (401239.6 1436192)
2  324. L     (352640.6 1490695)
3  158. L     (360954.9 1491590)
4  221. M     (291839.8 1466091)
5  209. M     (310991.9 1441630)
6  218. L     (304473.8 1512103)
\end{CodeOutput}
\end{CodeChunk}

\subsection{Model Fitting}\label{model-fitting}

SPGLMs in \pkg{spmodel} for point-referenced data are fit using the
\code{spglm()} function. The \code{spglm()} function requires four
arguments: \code{formula}, the relationship between the response and
explanatory variables; \code{family}, the response distribution assumed
for the response variable; \code{data}, the data frame that contains the
variables in \code{formula}, and \code{spcov_type}, the type of spatial
covariance. The \code{formula}, \code{family}, and \code{data} arguments
are the three required arguments to \code{glm()} for nonspatial GLMs.
So, the transition from \code{glm()} to \code{spglm()} simply requires
one additional argument: \code{spcov_type}. When \code{data} is not an
\code{sf} object, \code{spglm()} also requires the \code{xcoord} and
\code{ycoord} arguments, which indicate the columns in \code{data} that
represent the projected x- and y-coordinates, respectively.

We use \code{spglm()} to fit a SPGLM (i.e., here, a spatial logistic
regression model) quantifying the effect of elevation and strata on
moose presence:

\begin{CodeChunk}
\begin{CodeInput}
R> spbin <- spglm(
+   formula = presence ~ elev + strat,
+   family = binomial,
+   data = moose,
+   spcov_type = "exponential"
+ )
\end{CodeInput}
\end{CodeChunk}

The \code{summary()} function returns a model summary with relevant
information like the function call, deviance residuals, a coefficients
table of fixed effects, the pseudo R-squared, spatial covariance
parameters, and the GLM dispersion parameter (fixed at one in logistic
regression):

\begin{CodeChunk}
\begin{CodeInput}
R> summary(spbin)
\end{CodeInput}
\begin{CodeOutput}

Call:
spglm(formula = presence ~ elev + strat, family = binomial, data = moose, 
    spcov_type = "exponential")

Deviance Residuals:
    Min      1Q  Median      3Q     Max 
-1.7535 -0.8005  0.3484  0.7893  1.5797 

Coefficients (fixed):
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -2.465713   1.486212  -1.659 0.097104 .  
elev         0.006036   0.003525   1.712 0.086861 .  
stratM       1.439273   0.420591   3.422 0.000622 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared: 0.06275

Coefficients (exponential spatial covariance):
       de        ie     range 
5.145e+00 1.294e-03 4.199e+04 

Coefficients (Dispersion for binomial family):
dispersion 
         1 
\end{CodeOutput}
\end{CodeChunk}

The model provides some evidence that elevation is positively associated
with the log odds of moose presence (\(p~\)value \(\approx\) 0.087),
after controlling for strata. The model also provides strong evidence
that moose have a higher log odds of presence in the \code{"M"} strata
compared to the \code{"L"} strata (\(p~\)value \textless{} 0.001), after
controlling for elevation.

The fixed effects coefficients table from \code{summary()} is often of
primary scientific interest, but it is not immediately usable when
printed directly to the \proglang{R} console. The \code{tidy()} function
tidies this table, turning it into a data frame (i.e., a tibble) with
standard column names:

\begin{CodeChunk}
\begin{CodeInput}
R> tidy(spbin, conf.int = TRUE)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 3 x 7
  term        estimate std.error statistic  p.value conf.low conf.high
  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>
1 (Intercept) -2.47      1.49        -1.66 0.0971   -5.38e+0    0.447 
2 elev         0.00604   0.00353      1.71 0.0869   -8.73e-4    0.0129
3 stratM       1.44      0.421        3.42 0.000622  6.15e-1    2.26  
\end{CodeOutput}
\end{CodeChunk}

\subsection{Model Comparison}\label{model-comparison}

The strength of spatial covariance in the data affects how beneficial an
SPGLM is relative to a GLM. When the spatial covariance is strong, the
SPGLM should notably outperform the GLM. When the spatial covariance is
weak, the SPGLM and GLM should perform similarly. We can quantify the
benefits of incorporating spatial covariance for a particular data set
by comparing the fit of a SPGLM to a GLM. We can fit a GLM in
\code{spmodel} by specifying \code{spcov_type = "none"}:

\begin{CodeChunk}
\begin{CodeInput}
R> bin <- spglm(
+   formula = presence ~ elev + strat,
+   family = binomial,
+   data = moose,
+   spcov_type = "none"
+ )
\end{CodeInput}
\end{CodeChunk}

While the \code{spglm()} approach evaluates the HGLMM likelihood with
\(\sigma^2_{de} = 0\) and \(\sigma^2_{ie} \approx 0\) instead of just
the GLM likelihood, the parameter estimates and their standard errors
are the same:

\begin{CodeChunk}
\begin{CodeInput}
R> bin_glm <- glm(
+   formula = presence ~ elev + strat,
+   family = binomial,
+   data = moose,
+ )
R> round(coef(bin), digits = 4)
\end{CodeInput}
\begin{CodeOutput}
(Intercept)        elev      stratM 
    -0.4247     -0.0003      0.8070 
\end{CodeOutput}
\begin{CodeInput}
R> round(coef(bin_glm), digits = 4)
\end{CodeInput}
\begin{CodeOutput}
(Intercept)        elev      stratM 
    -0.4247     -0.0003      0.8070 
\end{CodeOutput}
\begin{CodeInput}
R> round(sqrt(diag(vcov(bin))), digits = 4)
\end{CodeInput}
\begin{CodeOutput}
(Intercept)        elev      stratM 
     0.4208      0.0019      0.2906 
\end{CodeOutput}
\begin{CodeInput}
R> round(sqrt(diag(vcov(bin_glm))), digits = 4)
\end{CodeInput}
\begin{CodeOutput}
(Intercept)        elev      stratM 
     0.4208      0.0019      0.2906 
\end{CodeOutput}
\end{CodeChunk}

However, using \code{spglm()} instead of \code{glm()} ensures that
\pkg{spmodel} helper functions are available and that each of the
\code{spglm()} models uses the same likelihood:

\begin{CodeChunk}
\begin{CodeInput}
R> glance(spbin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 10
      n     p  npar value   AIC  AICc   BIC logLik deviance
  <int> <dbl> <int> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>
1   218     3     3  676.  682.  683.  693.  -338.     176.
# i 1 more variable: pseudo.r.squared <dbl>
\end{CodeOutput}
\begin{CodeInput}
R> glance(bin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 10
      n     p  npar value   AIC  AICc   BIC logLik deviance
  <int> <dbl> <int> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>
1   218     3     0  708.  708.  708.  708.  -354.     294.
# i 1 more variable: pseudo.r.squared <dbl>
\end{CodeOutput}
\end{CodeChunk}

The likelihood-based statistics AIC, AICc, BIC, and deviance are much
lower for the SPGLM, indicating a better fit relative to the GLM. We may
also perform a likelihood ratio test (LRT) between the two models, as
the GLM is a special case of the SPGLM (i.e., is nested within the
SPGLM):

\begin{CodeChunk}
\begin{CodeInput}
R> anova(spbin, bin)
\end{CodeInput}
\begin{CodeOutput}
Likelihood Ratio Test

Response: presence
             Df   Chi2 Pr(>Chi2)    
spbin vs bin  3 31.546 6.525e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{CodeOutput}
\end{CodeChunk}

The LRT provides strong evidence that the SPGLM is preferred to the GLM
(\(p~\)value \textless{} 0.001).

An alternative approach to model comparison is to use a cross-validation
procedure \citep{james2013introduction}. The \code{loocv()} function
performs leave-one-out cross validation, comparing the predicted mean
(on the response scale) to the observed response variable for each
hold-out observation, recomputing estimates of \(\boldsymbol{\beta}\) in
each iteration. Performing leave-one-out cross validation tends to be
more computationally efficient than fitting the model, as leave-one-out
cross validation requires only one set of products involving the inverse
covariance matrix (a primary computational burden), while fitting
traditional models requires these products for each optimization
iteration. After performing leave-one-out cross validation, statistics
like bias, mean-squared-prediction error (MSPE), and the square root of
MSPE (RMSPE) can be used to evaluate models:

\begin{CodeChunk}
\begin{CodeInput}
R> loocv(spbin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 3
       bias  MSPE RMSPE
      <dbl> <dbl> <dbl>
1 0.0000206 0.156 0.394
\end{CodeOutput}
\begin{CodeInput}
R> loocv(bin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 3
      bias  MSPE RMSPE
     <dbl> <dbl> <dbl>
1 -1.23e-9 0.240 0.490
\end{CodeOutput}
\end{CodeChunk}

Both models have negligible bias, but the SPGLM has much lower MSPE and
RMSPE than the GLM, indicating the SPGLM predictions are far more
efficient. Three separate metrics (likelihood-based statistics,
likelihood-ratio test, and leave-one-out cross validation) prefer the
SPGLM to the GLM.

We can compare two SPGLMs with different spatial covariance functions
using likelihood-based statistics and leave-one-out cross validation,
but we can't use the LRT because generally, the spatial covariance
functions are not nested:

\begin{CodeChunk}
\begin{CodeInput}
R> spbin2 <- update(spbin, spcov_type = "gaussian")
R> glances(spbin, spbin2)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 2 x 11
  model      n     p  npar value   AIC  AICc   BIC logLik deviance
  <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>
1 spbin2   218     3     3  674.  680.  680.  690.  -337.     198.
2 spbin    218     3     3  676.  682.  683.  693.  -338.     176.
# i 1 more variable: pseudo.r.squared <dbl>
\end{CodeOutput}
\begin{CodeInput}
R> loocv(spbin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 3
       bias  MSPE RMSPE
      <dbl> <dbl> <dbl>
1 0.0000206 0.156 0.394
\end{CodeOutput}
\begin{CodeInput}
R> loocv(spbin2)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 3
       bias  MSPE RMSPE
      <dbl> <dbl> <dbl>
1 -0.000261 0.146 0.382
\end{CodeOutput}
\end{CodeChunk}

The \code{"exponential"} spatial covariance (\code{spbin}) has a
slightly lower (better) deviance but slightly higher (worse) AIC, AICc,
and BIC than the \code{"gaussian"} spatial covariance (\code{spbin2}).
Both spatial covariance functions have similar leave-one-out cross
validation metrics, though the \code{"gaussian"} spatial covariance
RMSPE is slightly lower (better). For practical purposes, these models
fit similarly.

Frequently in spatial statistics, the difference in model fit between
the best spatial model and worst spatial model is much smaller than the
difference in model fit between the worst spatial model and the
nonspatial model, implying that accounting for some form of spatial
covariance is very beneficial. Two spatial covariance functions to
consider starting with are the exponential and Gaussian, which have
quite different origin behaviors (Figure\(~\)\ref{fig-type}), something
\citet{stein1999interpolation} argues is important to characterize
accurately.

\subsection{Model Diagnostics}\label{model-diagnostics}

\code{spmodel} provides a suite of tools for model diagnostics. One is
\code{augment()}, which augments the data used in the model with several
model diagnostics (introduced in Section\(~\)\ref{sec:diagnostics}):

\begin{CodeChunk}
\begin{CodeInput}
R> augment(spbin)
\end{CodeInput}
\begin{CodeOutput}
Simple feature collection with 218 features and 8 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419057.4 ymax: 1541016
Projected CRS: NAD83 / Alaska Albers
# A tibble: 218 x 9
   presence  elev strat .fitted .resid    .hat  .cooksd .std.resid
 * <fct>    <dbl> <chr>   <dbl>  <dbl>   <dbl>    <dbl>      <dbl>
 1 0         469. L       -1.95 -0.516 0.0476  0.00465      -0.528
 2 0         362. L       -2.70 -0.361 0.0123  0.000548     -0.363
 3 0         173. M       -1.96 -0.514 0.00455 0.000405     -0.516
 4 0         280. L       -3.15 -0.290 0.00413 0.000117     -0.291
 5 0         620. L       -1.19 -0.728 0.168   0.0427       -0.798
 6 0         164. M       -1.71 -0.576 0.00534 0.000598     -0.578
 7 0         164. M       -1.60 -0.606 0.00576 0.000714     -0.608
 8 0         186. L       -2.50 -0.397 0.00439 0.000233     -0.398
 9 0         362. L       -1.88 -0.532 0.0239  0.00237      -0.539
10 0         430. L       -1.54 -0.623 0.0497  0.00713      -0.639
# i 208 more rows
# i 1 more variable: geometry <POINT [m]>
\end{CodeOutput}
\end{CodeChunk}

The fitted values (\code{.fitted}) can be returned on either the link
(\(\hat{\mathbf{w}}\)) or response (\(f^{-1}(\hat{\mathbf{w}})\)) scale
and the residuals (\code{.resid}) can be deviance, Pearson, or response
residuals. The default fitted values are on the link scale and the
default residuals are deviance residuals. Also returned by
\code{augment()} are the leverage (\code{.hat}), Cook's distance
(\code{.cooksd}), and standardized residuals (\code{.std.resid})
described in Section\(~\)\ref{sec:diagnostics}. A benefit of using
\code{augment()} when \code{data} is an \code{sf} object is that the
output is also an \code{sf} object, which makes it straightforward to
create spatial diagnostic plots (Figure\(~\)\ref{fig-sp-diagnostic}).
Standard \proglang{R} helpers (e.g., \code{fitted()},
\code{residuals()}) are also available to extract model diagnostics from
the model object.

\begin{figure}
\centering
\includegraphics[width = 1\linewidth]{figures/figure-2.png}
\caption{Moose presence model diagnostics, including leverage values (left) and standardized residuals (right).}
\label{fig-sp-diagnostic}
\end{figure}

The \code{plot()} function can also be used to return similar
diagnostics as from \code{lm()} and \code{glm()}, with additional tools
for diagnosing spatial covariance. For example, we can inspect Cook's
distance values and the empirical spatial covariance as a function of
distance with (Figure\(~\)\ref{fig-sp-diagnostic2}):

\begin{CodeChunk}
\begin{CodeInput}
R> plot(spbin, which = c(4, 7))
\end{CodeInput}
\end{CodeChunk}

\begin{figure}
\centering
\includegraphics[width = 1\linewidth]{figures/figure-3.png}
\caption{Moose presence model diagnostics, including Cook's distance (left) and the fitted spatial covariance as a function of distance (right).}
\label{fig-sp-diagnostic2}
\end{figure}

The \code{varcomp()} function partitions model variability into several
different components, helping to elucidate the model's structure:

\begin{CodeChunk}
\begin{CodeInput}
R> varcomp(spbin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 3 x 2
  varcomp            proportion
  <chr>                   <dbl>
1 Covariates (PR-sq)   0.0627  
2 de                   0.937   
3 ie                   0.000236
\end{CodeOutput}
\end{CodeChunk}

The pseudo R-squared (\(PR^2\)) is reported in the first row. The
remaining variability (\(1 - PR^2\)) is allocated proportionally to
\code{de} and \code{ie} according to \(\sigma^2_{de}\) and
\(\sigma^2_{ie}\). This variability partitioning is a useful tool that
helps quantify how much the explanatory variables, residual spatial
variance, and residual nonspatial variance contribute to model fit; as
with \(PR^2\), it should not be used for model comparison, but rather as
a helpful model diagnostic.

\subsection{Prediction}\label{prediction}

We can predict the probability of moose presence at the locations in
\code{moose_preds} using \code{predict()}:

\begin{CodeChunk}
\begin{CodeInput}
R> predict(spbin, newdata = moose_preds)[1:5]
\end{CodeInput}
\begin{CodeOutput}
          1           2           3           4           5 
 0.06664165 -0.79069107 -1.60387940 -0.83159357  1.38183928 
\end{CodeOutput}
\end{CodeChunk}

By default, predictions are returned on the link scale, but this can be
changed to the response scale via \code{type}:

\begin{CodeChunk}
\begin{CodeInput}
R> predict(spbin, newdata = moose_preds, type = "response")[1:5]
\end{CodeInput}
\begin{CodeOutput}
        1         2         3         4         5 
0.5166542 0.3120203 0.1674401 0.3033082 0.7992862 
\end{CodeOutput}
\end{CodeChunk}

\begin{figure}
\centering
\includegraphics[width = 0.70\linewidth]{figures/figure-4.png}
\caption{Moose presence probability fitted values and predictions. Fitted values are represented by circles and predictions by triangles.}
\label{fig-moose-fit}
\end{figure}

Predictions on the response scale are visualized alongside the fitted
values (\(f^{-1}(\hat{\mathbf{w}})\)) in Figure\(~\)\ref{fig-moose-fit}.
Prediction intervals for the probability of moose presence (on the link
scale) are returned by supplying \code{interval}:

\begin{CodeChunk}
\begin{CodeInput}
R> predict(spbin, newdata = moose_preds, interval = "prediction")[1:5, ]
\end{CodeInput}
\begin{CodeOutput}
          fit        lwr       upr
1  0.06664165 -2.0374370 2.1707203
2 -0.79069107 -3.4758514 1.8944692
3 -1.60387940 -4.0953329 0.8875741
4 -0.83159357 -3.0704818 1.4072947
5  1.38183928 -0.7692107 3.5328893
\end{CodeOutput}
\end{CodeChunk}

We can alternatively use \code{augment()} to augment the prediction data
with predictions. Arguments to \code{predict()} can also be passed to
\code{augment()}:

\begin{CodeChunk}
\begin{CodeInput}
R> augment(spbin, newdata = moose_preds, interval = "prediction")
\end{CodeInput}
\begin{CodeOutput}
Simple feature collection with 100 features and 5 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 269386.2 ymin: 1418453 xmax: 419976.2 ymax: 1541763
Projected CRS: NAD83 / Alaska Albers
# A tibble: 100 x 6
    elev strat .fitted .lower  .upper           geometry
 * <dbl> <chr>   <dbl>  <dbl>   <dbl>        <POINT [m]>
 1  143. L      0.0666 -2.04   2.17   (401239.6 1436192)
 2  324. L     -0.791  -3.48   1.89   (352640.6 1490695)
 3  158. L     -1.60   -4.10   0.888  (360954.9 1491590)
 4  221. M     -0.832  -3.07   1.41   (291839.8 1466091)
 5  209. M      1.38   -0.769  3.53   (310991.9 1441630)
 6  218. L     -2.59   -5.20   0.0177 (304473.8 1512103)
 7  127. L     -2.73   -5.24  -0.220  (339011.1 1459318)
 8  122. L     -2.32   -4.74   0.0920 (342827.3 1463452)
 9  191  L     -1.17   -4.01   1.66   (284453.8 1502837)
10  105. L     -0.905  -3.05   1.24   (391343.9 1483791)
# i 90 more rows
\end{CodeOutput}
\end{CodeChunk}

By using \code{augment()} when \code{newdata} is an \code{sf} object,
predictions and their corresponding uncertainties are readily available
for spatial mapping (Figure\(~\)\ref{fig-moose-int}).

\begin{figure}
\centering
\includegraphics[width = 1\linewidth]{figures/figure-5.png}
\caption{Moose presence 95\% prediction interval lower bounds (left) and upper bounds (right).}
\label{fig-moose-int}
\end{figure}

\section{Additional applications}\label{sec-applications2}

Throughout the remainder of this section, we briefly highlight some
additional \pkg{spmodel} capabilities for SPGLMs. In
Section\(~\)\ref{sec-moose-count}, we fit Poisson and negative binomial
models with and without geometric anisotropy for the point-referenced
moose count data. In Section\(~\)\ref{sec-lake}, we fit a Gamma model to
the point-referenced lake conductivity data, showing how to fit a model
with a partition factor, perform a spatial analysis of variance (ANOVA),
and estimate contrasts for models with interactions. In
Section\(~\)\ref{sec-seal}, we fit a binomial model to the areal harbor
seal trend data with a nonspatial random effect. Finally in
Section\(~\)\ref{sec-texas}, we fit beta models to Texas voter turnout
data, which can be treated as point-referenced or areal, and use maximum
likelihood to compare two models with different explanatory variables.
Table\(~\)\ref{tab:additional} outlines, for each application, the
section number, data set, family (i.e., response distribution), geometry
type (point-referenced or areal), and additional \pkg{spmodel} features
highlighted.

\begin{table}
    \centering
    \begin{tabular}{|l|llll|}
    \hline
    Section & Data & Family & Geometry & Additional Features \\
    \hline
    \hline
    4.1 & Moose Counts & Poisson & Point & Geometric Anisotropy  \\
    &  & NBinomial &  &  \\
    \hline
    4.2 & Lake Conductivity & Gamma & Point & Partition Factor  \\
    & & & & ANOVA \\
    & & & & Contrasts \\
    \hline
    4.3 & Harbor Seals & Binomial & Areal & Nonspatial Random Effects  \\
    \hline
    4.4 & Texas Voter Turnout & Beta & Point & Likelihood-Ratio Test  \\
    & & & Areal & \\
    \hline 
    \end{tabular}
\caption{Section number, data set, family, geometry type, and additional features for each application.}
\label{tab:additional}
\end{table}

\subsection{Modeling moose counts in Alaska, USA}\label{sec-moose-count}

In addition to moose presence, moose counts are also recorded in
\code{moose} (Figure\(~\)\ref{fig-moose-data-count}). The Poisson and
negative binomial response distributions can be used to model SPGLMs for
count data. The Poisson distribution mean is equal to its variance,
while the negative binomial has an extra parameter to accommodate
overdispersion (where the variance is larger than the mean). Using a
spherical spatial covariance function, we may fit both a Poisson and
negative binomial SPGLM changing the \code{family} argument:

\begin{CodeChunk}
\begin{CodeInput}
R> sppois <- spglm(
+   formula = count ~ elev + strat,
+   family = poisson,
+   data = moose,
+   spcov_type = "spherical"
+ )
R> spnb <- update(sppois, family = nbinomial)
\end{CodeInput}
\end{CodeChunk}

\begin{figure}
\centering
\includegraphics[width = 0.70\linewidth]{figures/figure-6.png}
\caption{Moose counts in Alaska. Circles represent moose counts (based on color) and triangles represent locations at which mean count predictions are desired.}
\label{fig-moose-data-count}
\end{figure}

Because the Poisson and negative binomial distributions have the same
response support (nonnegative integers), we can compare them using AIC,
AICc, or BIC:

\begin{CodeChunk}
\begin{CodeInput}
R> BIC(sppois, spnb)
\end{CodeInput}
\begin{CodeOutput}
       df      BIC
sppois  3 1344.574
spnb    4 1343.105
\end{CodeOutput}
\end{CodeChunk}

Implicit in our spatial covariance functions thus far has been an
assumption of geometric isotropy. A spatial covariance function is
geometrically isotropic if it decays with distance at the same rate in
all directions (Figure\(~\)\ref{fig-tropy}; left). A spatial covariance
is geometrically anisotropic if it decays with distance at different
rates in different directions (Figure\(~\)\ref{fig-tropy}; right).
Geometric anisotropy is formally incorporated by rotating and scaling
original coordinates, yielding transformed coordinates that are
geometrically isotropic: \begin{align}
  \begin{bmatrix}
    x^* \\
    y^*
  \end{bmatrix} = 
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 / \omega
  \end{bmatrix}
  \begin{bmatrix}
    \cos(\alpha) & \sin(\alpha) \\
    -\sin(\alpha) & \cos(\alpha)
  \end{bmatrix}  
  \begin{bmatrix}
    x \\
    y
  \end{bmatrix}.
\end{align} The parameters \(\omega\) and \(\alpha\) controls the
scaling and rotation, respectively, of the major and minor axes of a
level curve of equal spatial covariance (Figure\(~\)\ref{fig-tropy}).
Using these transformed coordinates, the partial sill
(\(\sigma^2_{de}\)), nugget (\(\sigma^2_{ie}\)), and range (\(\phi\))
parameters are estimated. We accommodate geometric anisotropy by
supplying \code{anisotropy}:

\begin{CodeChunk}
\begin{CodeInput}
R> sppois_anis <- update(sppois, anisotropy = TRUE)
R> spnb_anis <- update(spnb, anisotropy = TRUE)
\end{CodeInput}
\end{CodeChunk}

According to BIC, the spatial negative binomial model with geometric
anisotropy performs best:

\begin{CodeChunk}
\begin{CodeInput}
R> BIC(sppois, spnb, sppois_anis, spnb_anis)
\end{CodeInput}
\begin{CodeOutput}
            df      BIC
sppois       3 1344.574
spnb         4 1343.105
sppois_anis  5 1341.143
spnb_anis    6 1339.714
\end{CodeOutput}
\end{CodeChunk}

The \code{plot()} function can be used to visualize the anisotropy
(Figure\(~\)\ref{fig-tropy}):

\begin{CodeChunk}
\begin{CodeInput}
R> plot(spnb, which = 8)
R> plot(spnb_anis, which = 8)
\end{CodeInput}
\end{CodeChunk}

The spatial covariance is strongest in a northwest-southeast direction
and weakest in the northeast-southwest direction
(Figure\(~\)\ref{fig-tropy}), which is intuitive given the similar
patterns in moose counts from Figure\(~\)\ref{fig-moose-data-count}.

\begin{figure}[h]
\centering
\includegraphics[width = 1\linewidth]{figures/figure-7.png}
\caption{Level curves of equal spatial covariance for the negative binomial moose count models. The ellipse is centered at zero distance in the x-direction and y-direction, and points along the ellipse have equal levels of spatial covariance.  In the isotropic level curve (left), spatial covariance decays equally in all directions. In the anistropic level curve (right), spatial covariance decays fastest in the northeast-southwest direction and slowest in the northwest-southeast direction (this pattern can be seen in the observed counts).}
\label{fig-tropy}
\end{figure}

\subsection{Modeling lake conductivity in Southwest,
USA}\label{sec-lake}

The \code{lake} data in \code{spmodel} contains climate and chemical
data for several lakes in four southwestern states in the United States:
Arizona, Colorado, Nevada, and Utah. We desire an SPGLM that
characterizes the effect of temperature, state, and lake origin (whether
the lake is naturally occurring or human made) on lake conductivity.
Conductivity is a measure of dissolved ions (measured here in water),
which is important for various physical, chemical, and biological
processes. Chemical data are often heavily right-skewed, so we model
them using an SPGLM assuming a Gamma distribution for the response. The
\code{log_cond} variable in \code{lake} is the logarithm of
conductivity, which we dynamically exponentiate within \code{formula} so
that it is on the original scale:

\begin{CodeChunk}
\begin{CodeInput}
R> spgam <- spglm(
+   formula = exp(log_cond) ~ temp * state + origin, 
+   family = "Gamma",
+   data = lake,
+   spcov_type = "cauchy",
+   partition_factor = ~ year
+ )
\end{CodeInput}
\end{CodeChunk}

We model conductivity as a function of temperature, state, and lake
origin, and we allow the effect of temperature to vary by state
(\code{temp:state} interaction). The \code{year} partition factor
(specified via \code{partition_factor}) restricts spatial covariance to
be nonzero only for observations sampled during the same year. Data were
collected in 2012 and 2017, so this partition factor assumes
independence between observations in 2012 and 2017. While we used the
partition factor here illustratively, more generally, the utility of
partition factors can be highly context dependent.

When categorical variables have more than two levels, the default
reference group contrasts are not well-suited to assess the variable's
overall significance:

\begin{CodeChunk}
\begin{CodeInput}
R> summary(spgam)
\end{CodeInput}
\begin{CodeOutput}

Call:
spglm(formula = exp(log_cond) ~ temp * state + origin, family = "Gamma", 
    data = lake, spcov_type = "cauchy", partition_factor = ~year)

Deviance Residuals:
     Min       1Q   Median       3Q      Max 
-1.35762 -0.20796 -0.03706  0.17869  1.10616 

Coefficients (fixed):
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    3.59325    0.50058   7.178 7.06e-13 ***
temp           0.15182    0.03006   5.051 4.39e-07 ***
stateCO       -0.03214    0.56098  -0.057  0.95432    
stateNV        0.75664    0.66851   1.132  0.25771    
stateUT       -0.19696    0.55916  -0.352  0.72466    
originNATURAL  0.08313    0.21988   0.378  0.70538    
temp:stateCO   0.13679    0.04808   2.845  0.00444 ** 
temp:stateNV   0.01882    0.05820   0.323  0.74645    
temp:stateUT   0.20015    0.04846   4.131 3.62e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared: 0.7061

Coefficients (cauchy spatial covariance):
       de        ie     range     extra 
2.069e-02 2.952e-01 4.119e+06 5.645e-01 

Coefficients (Dispersion for Gamma family):
dispersion 
     3.761 
\end{CodeOutput}
\end{CodeChunk}

A more effective approach is to use an analysis of variance (ANOVA),
which is well-suited to assess the overall significance of each
variable:

\begin{CodeChunk}
\begin{CodeInput}
R> anova(spgam)
\end{CodeInput}
\begin{CodeOutput}
Analysis of Variance Table

Response: exp(log_cond)
            Df    Chi2 Pr(>Chi2)    
(Intercept)  1 51.5270 7.062e-13 ***
temp         1 25.5146 4.390e-07 ***
state        3  3.0747 0.3802528    
origin       1  0.1429 0.7053819    
temp:state   3 19.7668 0.0001897 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{CodeOutput}
\end{CodeChunk}

The main effect for temperature and the temperature by state interaction
are highly significant (\(p~\)value \textless{} 0.001), while the main
effects for state and lake origin are not significant.

Variance inflation factors assess the degree to which standard errors
\(\hat{\boldsymbol{\beta}}\) are inflated due to covariance among the
columns of \(\mathbf{X}\). Generalized variance inflation factors can
capture the variance inflation for subsets of \(\mathbf{X}\) that may
include categorical variables with more than two levels
\citep{fox1992generalized}:

\begin{CodeChunk}
\begin{CodeInput}
R> library("car")
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> vif(spgam)
\end{CodeInput}
\begin{CodeOutput}
                 GVIF Df GVIF^(1/(2*Df))
temp         4.691914  1        2.166083
state      127.082397  3        2.242234
origin       1.264940  1        1.124695
temp:state  76.387383  3        2.059856
\end{CodeOutput}
\end{CodeChunk}

The GVIF\(^{1/2df}\) values for \code{temp}, \code{state}, and
\code{temp:state} are just greater than two, which suggests moderate
multicollinearity for these terms -- unsurprising given the
\code{temp:state} interaction in the model. The GVIF\(^{1/2df}\) for
\code{origin} is close to one, which suggests little to no
multicollinearity for this term.

Because of the interaction between \code{temp} and \code{state},
contrasts that assess mean differences among states should condition
upon a specific temperature value. By default, \pkg{emmeans} uses the
mean temperature value (here, 7.63) to assess contrasts:

\begin{CodeChunk}
\begin{CodeInput}
R> library("emmeans")
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> pairs(emmeans(spgam, ~ state | temp))
\end{CodeInput}
\begin{CodeOutput}
temp = 7.63:
 contrast estimate    SE  df z.ratio p.value
 AZ - CO    -1.012 0.337 Inf  -3.004  0.0142
 AZ - NV    -0.900 0.348 Inf  -2.584  0.0480
 AZ - UT    -1.331 0.326 Inf  -4.082  0.0003
 CO - NV     0.112 0.258 Inf   0.434  0.9727
 CO - UT    -0.319 0.223 Inf  -1.427  0.4822
 NV - UT    -0.431 0.244 Inf  -1.763  0.2915

Results are averaged over the levels of: origin 
Degrees-of-freedom method: asymptotic 
Results are given on the log (not the response) scale. 
P value adjustment: tukey method for comparing a family of 4 estimates 
\end{CodeOutput}
\end{CodeChunk}

Again, because of the interaction between \code{temp} and \code{state},
we should assess temperature trends separately for each state:

\begin{CodeChunk}
\begin{CodeInput}
R> emtrends(spgam, ~ state, var = "temp")
\end{CodeInput}
\begin{CodeOutput}
 state temp.trend     SE  df asymp.LCL asymp.UCL
 AZ         0.152 0.0301 Inf    0.0929     0.211
 CO         0.289 0.0370 Inf    0.2161     0.361
 NV         0.171 0.0504 Inf    0.0718     0.270
 UT         0.352 0.0372 Inf    0.2791     0.425

Results are averaged over the levels of: origin 
Degrees-of-freedom method: asymptotic 
Results are given on the exp (not the response) scale. 
Confidence level used: 0.95 
\end{CodeOutput}
\end{CodeChunk}

\subsection{Modeling harbor seal trends in Alaska, USA}\label{sec-seal}

The \code{seal} data in \pkg{spmodel} contains harbor seal abundance
trends for two different harbor seal stocks (genetically distinct
populations). While the \code{moose} and \code{lake} data were
point-referenced, the \code{seal} data are areal. Each polygon in the
\code{seal} data represents a distinct harbor seal haulout region
(Figure\(~\)\ref{fig-seal}). A haulout region is an area of coastal
rocks that harbor seals go to rest, molt, and give birth.

\begin{figure}
\centering
\includegraphics[width = 1\linewidth]{figures/figure-8.png}
\caption{Seal trend distribution in Alaska. Observed and missing seal polygons by stock (left) and observed log seal trends (right).}
\label{fig-seal}
\end{figure}

For each polygon, a Poisson regression was used to quantify the mean
trend in abundance over approximately 30 years \citep{ver2018spatial}.
If the logarithm of mean abundance trends (\code{log_trend}) is negative
(positive), it means abundance is decreasing (increasing). We use a
binomial SPGLM to quantify the likelihood that mean abundance trends are
decreasing:

\begin{CodeChunk}
\begin{CodeInput}
R> is_decreasing <- seal$log_trend < 0
R> spbin <- spgautor(
+   formula = is_decreasing ~ 1,
+   family = binomial,
+   data = seal,
+   spcov_type = "car",
+   random = ~ stock
+ )
\end{CodeInput}
\end{CodeChunk}

To model spatial dependence, we used a conditional autoregressive
function. Conditional and simultaneous autoregressive functions
characterize spatial distance through neighborhood relationships (rather
than Euclidean distance) and have \code{spcov_type} values of
\code{"car"} and \code{"sar"}, respectively. By default, Queen's
distance is used to determine whether two sites are neighbors, though
custom neighborhood matrices can be passed via \code{W}. Row
standardization is also assumed by default; this can be changed via
\code{row_st}. Using \code{random}, we also specified a nonspatial
random effect for seal stock, which implies seals belonging to the same
stock share extra covariance. The \code{random} argument uses similar
syntax as \pkg{lme4} \citep{bates2015lme4} and \pkg{nlme}
\citep{pinheiro2006mixed} to specify nonspatial random effects.

Tidying the model reveals the estimates and confidence intervals on the
log odds scale:

\begin{CodeChunk}
\begin{CodeInput}
R> tidy(spbin, conf.int = TRUE)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 7
  term        estimate std.error statistic p.value conf.low conf.high
  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>
1 (Intercept)    0.340     0.673     0.506   0.613   -0.979      1.66
\end{CodeOutput}
\end{CodeChunk}

Back-transforming the confidence interval to the probability scale
yields:

\begin{CodeChunk}
\begin{CodeInput}
R> emmeans(spbin, ~ 1, type = "response")
\end{CodeInput}
\begin{CodeOutput}
 1        prob    SE  df asymp.LCL asymp.UCL
 overall 0.584 0.164 Inf     0.273      0.84

Degrees-of-freedom method: asymptotic 
Confidence level used: 0.95 
Intervals are back-transformed from the logit scale 
\end{CodeOutput}
\end{CodeChunk}

The \code{SE} column is the standard error on the response scale
obtained from the delta method \citep{oehlert1992note, ver2012invented}.

In contrast to point-referenced data, prediction locations for areal
data must be specified at the time of model fitting, as they affect the
spatial covariance function's neighborhood structure. Prediction
locations whose response values have an \code{NA} (i.e., missing) value
are converted into a \code{newdata} object that is stored in the model
output. For example, rows one and nine are locations without seal
trends, meaning they are not used in model fitting but are desired for
prediction:

\begin{CodeChunk}
\begin{CodeInput}
R> seal
\end{CodeInput}
\begin{CodeOutput}
Simple feature collection with 149 features and 2 fields
Geometry type: POLYGON
Dimension:     XY
Bounding box:  xmin: 913618.8 ymin: 855730.2 xmax: 1221859 ymax: 1145054
Projected CRS: NAD83 / Alaska Albers
# A tibble: 149 x 3
   log_trend stock                                            geometry
 *     <dbl> <fct>                                       <POLYGON [m]>
 1  NA       8     ((1035002 1054710, 1035002 1054542, 1035002 105354~
 2  -0.282   8     ((1037002 1039492, 1037006 1039490, 1037017 103949~
 3  -0.00121 8     ((1070158 1030216, 1070185 1030207, 1070187 103020~
 4   0.0354  8     ((1054906 1034826, 1054931 1034821, 1054936 103482~
 5  -0.0160  8     ((1025142 1056940, 1025184 1056889, 1025222 105683~
 6   0.0872  8     ((1026035 1044623, 1026037 1044605, 1026072 104461~
 7  -0.266   8     ((1100345 1060709, 1100287 1060706, 1100228 106070~
 8   0.0743  8     ((1030247 1029637, 1030248 1029637, 1030265 102964~
 9  NA       8     ((1043093 1020553, 1043097 1020550, 1043101 102055~
10  -0.00961 8     ((1116002 1024542, 1116002 1023542, 1116002 102254~
# i 139 more rows
\end{CodeOutput}
\end{CodeChunk}

Then, \code{predict()} can be called without having to specify
\code{newdata}:

\begin{CodeChunk}
\begin{CodeInput}
R> predict(spbin, type = "response", interval = "prediction")[1:5, ]
\end{CodeInput}
\begin{CodeOutput}
         fit       lwr       upr
1  0.6807677 0.3863736 0.8783808
9  0.5945680 0.2467634 0.8678078
13 0.6189055 0.2974432 0.8616799
15 0.6040102 0.2921802 0.8493132
18 0.6375700 0.3356282 0.8596641
\end{CodeOutput}
\end{CodeChunk}

We could have alternatively used a (geostatistical) SPGLM via
\code{spglm()}. When areal data are used with \code{spglm()}, the
centroids of each polygon are used as the point-referenced coordinates.
We further explore comparisons between point-referenced and areal data
in the next example.

\subsection{Modeling voter turnout in Texas, USA}\label{sec-texas}

\begin{figure}
\centering
\includegraphics[width = 0.70\linewidth]{figures/figure-10.png}
\caption{Proportion of voter turnout in Texas for the 1980 presidential election. Circles represent voter turnout (based on color) and triangles represent locations at which voter turnout predictions are desired.}
\label{fig-texas}
\end{figure}

The \code{texas} data in \pkg{spmodel} contains voter turnout data for
Texas counties in the 1980 United States Presidential Election
\citep{bivand2024spdata}. The data are point-referenced, with polygon
centroids representing the spatial location of each county
(Figure\(~\)\ref{fig-texas}). Beta regression is a GLM used to model
rate and proportion data in the (0, 1) interval
\citep{ferrari2004beta, cribari2010beta}. We model voter turnout rates
as a function of mean log income of county residents using an SPGLM
assuming a beta distributed response variable:

\begin{CodeChunk}
\begin{CodeInput}
R> spbeta_geo <- spglm(
+   formula = turnout ~ log_income,
+   family = "beta", 
+   data = texas,
+   spcov_type = "matern"
+ )
\end{CodeInput}
\end{CodeChunk}

Alternatively, we could use an autoregressive model to fit the model,
constructing a neighborhood matrix by assuming centroids within
\code{cutoff} of one another are neighbors:

\begin{CodeChunk}
\begin{CodeInput}
R> spbeta_auto <- spgautor(
+   formula = turnout ~ log_income,
+   family = "beta", 
+   data = texas,
+   spcov_type = "car",
+   cutoff = 1e5
+ )
\end{CodeInput}
\end{CodeChunk}

According to AIC, the SPGLM for point-referenced data is preferred:

\begin{CodeChunk}
\begin{CodeInput}
R> AIC(spbeta_geo, spbeta_auto)
\end{CodeInput}
\begin{CodeOutput}
            df       AIC
spbeta_geo   5 -44.53113
spbeta_auto  3 -22.46104
\end{CodeOutput}
\end{CodeChunk}

The default estimation method in \pkg{spmodel} for SPGLMs is restricted
maximum likelihood (REML), while maximum likelihood (ML) can also be
used. A benefit of benefit of REML is that it can yield unbiased
estimates of covariance parameters \citep{cressie1993asymptotic}, but a
drawback is that likelihood-based statistics are only valid for model
comparison when the models have the same explanatory variable and fixed
effect structure (because the error contrasts used to construct the REML
likelihood change based on \(\mathbf{X}\) and \(\boldsymbol{\beta}\)).
In contrast, ML estimators are generally biased for covariance
parameters, though in practice this bias tends to be small. Moreover,
when using ML, likelihood-based comparisons are valid for models having
different explanatory variable and fixed effect structures. Using ML, we
can evaluate the significance of log income on voter turnout using a
likelihood ratio test:

\begin{CodeChunk}
\begin{CodeInput}
R> spbeta_full_ml <- update(spbeta_geo, estmethod = "ml")
R> spbeta_red_ml <- update(spbeta_geo, estmethod = "ml", formula = turnout ~ 1)
R> anova(spbeta_full_ml, spbeta_red_ml)
\end{CodeInput}
\begin{CodeOutput}
Likelihood Ratio Test

Response: turnout
                                Df   Chi2 Pr(>Chi2)    
spbeta_red_ml vs spbeta_full_ml  1 23.155 1.494e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{CodeOutput}
\end{CodeChunk}

The likelihood ratio test provides strong evidence that log income is
significantly related to voter turnout (\(p~\)value \textless{} 0.001).
Alternatively, we could have instead used a different likelihood-based
statistic like AIC:

\begin{CodeChunk}
\begin{CodeInput}
R> AIC(spbeta_full_ml, spbeta_red_ml)
\end{CodeInput}
\begin{CodeOutput}
               df       AIC
spbeta_full_ml  7 -31.25900
spbeta_red_ml   6 -10.10354
\end{CodeOutput}
\end{CodeChunk}

The AIC also prefers the full model, suggesting that log income is
important for predicting voter turnout.

\section{Discussion}\label{sec-discussion}

SPGLMs are fit in \pkg{spmodel} using a novel application of the Laplace
approximation that simultaneously marginalizes over the latent (i.e.,
unobserved) random effects and the fixed effects. \pkg{spmodel}'s
\code{spglm()} (for point-referenced data) and \code{spgautor()} (for
areal data) fit SPGLMs that are similar in structure and syntax as base
\proglang{R}'s \code{glm()} function, easing the transition from GLMs to
SPGLMs for practitioners. The \code{spglm()} and \code{spgautor()}
functions support six response distributions for binary, count, and
skewed data and 20 spatial covariance functions. \pkg{spmodel} has a
suite of tools for data visualization, inference, model diagnostics, and
prediction, providing a framework that can be used for all stages of a
data analysis. There are many additional \pkg{spmodel} features that are
not covered here, including fitting multiple models simultaneously,
fixing spatial covariance and dispersion parameters at known values,
fitting models to large non-Gaussian data having thousands of
observations via spatial indexing \citep{ver2023indexing}, incorporating
spatial dependence in machine learning (e.g., random forests;
\citet{breiman2001random}), simulating spatially dependent data (e.g.,
\code{spbinom()}, \code{sprpois()}, etc.), and more. Further details are
provided by \url{https://CRAN.R-project.org/package=spmodel} and links
therein.

\section*{Data and code availability}\label{data-and-code-availability}
\addcontentsline{toc}{section}{Data and code availability}

The results in this manuscript were obtained using {R} 4.4.0 with the
\pkg{spmodel} 0.11.1 package. Figures were created using the {ggplot2}
3.5.1 package \citep{wickham2016ggplot2} and base {R}.

All writing and code associated with this manuscript is available for
viewing and download on GitHub at
\url{https://github.com/USEPA/spmodel.glm.manuscript}. All data used are
part of the \pkg{spmodel} {R} package available for download from CRAN
at \url{https://CRAN.R-project.org/package=spmodel}. Results were
obtained using {R} 4.4.0 with the \pkg{spmodel} 0.11.1 package. Figures
were created using the {ggplot2} 3.5.1 package
\citep{wickham2016ggplot2} and base {R}.

\section*{Acknowledgments}\label{acknowledgments}
\addcontentsline{toc}{section}{Acknowledgments}

We would like to genuinely thank the associate editor, anonymous
reviewers, and editorial staff for significant support and feedback that
greatly improved the manuscript.

The views expressed in this article are those of the author(s) and do
not necessarily represent the views or policies of the U.S. government,
U.S. Environmental Protection Agency or the National Oceanic and
Atmospheric Administration. Mention of trade names or commercial
products does not constitute endorsement or recommendation for use.

\bibliography{references.bib}




\end{document}
