\documentclass[
]{jss}

%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern}

\usepackage[utf8]{inputenc}

\author{
Michael Dumelle~\orcidlink{0000-0002-3393-5529}\\United States\\
Environmental Protection Agency \And Jay M. Ver
Hoef~\orcidlink{0000-0003-4302-6895}\\Alaska Fisheries\\
Science Center \And Matt
Higham~\orcidlink{0009-0006-4217-625X}\\St.~Lawrence University
}
\title{Spatial Generalized Linear Models in \proglang{R} Using
\pkg{spmodel}}

\Plainauthor{Michael Dumelle, Jay M. Ver Hoef, Matt Higham}
\Plaintitle{Spatial Generalized Linear Models in R Using spmodel}
\Shorttitle{Spatial Generalized Linear Models in \proglang{R} Using
\pkg{spmodel}}


\Abstract{
Generalized linear models (GLMs) describe a non-normal response variable
that may be binary, count, skewed, or a proportion. Typically,
observations in a GLM are assumed independent of one another. For
spatial data, this independence assumption is impractical, as nearby
locations tend to be more similar than locations far apart. The
\pkg{spmodel} \proglang{R} package provides tools to fit GLMs that
incorporate spatial correlation (i.e., spatial generalized linear
models, or SPGLMs). SPGLMs are fit in \pkg{spmodel} using a novel
application of the Laplace approximation via \code{spglm()} for
point-referenced data or \code{spgautor()} for areal (i.e., lattice),
data. \code{spglm()} and \code{spgautor()} closely resemble {glm} from
base \proglang{R} but include arguments that control the spatial
correlation structure. \pkg{spmodel} has many helper functions for model
inspection and diagnostics, some of which leverage other \proglang{R}
packages like {broom} and {emmeans}. \pkg{spmodel} has tools to make
predictions of the latent spatial-mean process at unobserved locations.
\pkg{spmodel} also provides many advanced features like accommodating
geometric anisotropy and nonspatial random effects, simulating spatially
autocorrelated data, and more. Here we use \pkg{spmodel} to illustrate
the modeling of binary, count, skewed and proportion response variables
from several point-referenced and areal data sets.
}

\Keywords{autoregressive model, geostatistical model, spatial
covariance, spatial correlation}
\Plainkeywords{autoregressive model, geostatistical model, spatial
covariance, spatial correlation}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
    Michael Dumelle\\
    United States\\
Environmental Protection Agency\\
    200 SW 35th St\\
Corvallis, OR, 97330\\
  E-mail: \email{Dumelle.Michael@epa.gov}\\
  
      }


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}




\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm, bbm}
\usepackage{lineno}
\usepackage{caption, subcaption}

\begin{document}



\newpage

\section{Introduction}\label{sec-intro}

In practice, non-Gaussian data are ubiquitous. Non-Gaussian that belong
to an exponential family data can be naturally modeled using a
generalized linear model (GLM) regression framework
\citep{nelder1972generalized, mccullagh1989generalized, myers2012generalized, faraway2016extending}.
In a GLM, an \(n \times 1\) response variable \(\mathbf{y}\) belongs to
a a statistical distribution (e.g., Poisson, Binomial) with some mean
and variance. Often, the analysis goal is to study the impact of a
linear function of several explanatory variables on \(\text{y}\) through
a GLM. In this context, the latent (i.e., unobserved) mean of
\(\mathbf{y}\), \(\boldsymbol{\mu}\), is linked to these explanatory
variables via a link function: \begin{equation}\label{eq-glm}
f(\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta}) \equiv \mathbf{w} = \mathbf{X} \boldsymbol{\beta},
\end{equation} where for a sample size \(n\), \(f(\cdot)\) is a link
function that connects \(\boldsymbol{\mu}\) to \(\mathbf{w}\),
\(\mathbf{X}\) is the \(n \times p\) design matrix of explanatory
variables, and \(\boldsymbol{\beta}\) is the \(p \times 1\) vector of
fixed effects. While the mean is typically constrained in some way
(e.g., between zero and one if a probability), the link function
generally makes \(\mathbf{w}\) is unconstrained. Common link functions
inlude the log odds (i.e., logit) link for binary and proprtion data and
the log link count and skewed data. Equation\(~\)\ref{eq-glm} can also
be written in terms of the inverse link function, \(f^{-1}(\cdot)\):
\begin{equation}\label{eq-glm2}
\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta} \equiv f^{-1}(\mathbf{w}) = f^{-1}(\mathbf{X} \boldsymbol{\beta}),
\end{equation}

The GLM fixed effects (\(\boldsymbol{\beta}\)) are typically estimated
via maximum likelihood \citep{chambers1992S}. It is often convenient to
compute the maximum likelihood estimates using the iteratively
reweighted least squares (IRWLS) algorithm \citep{wood2017generalized},
which is the approach used by the \code{glm()} function in the
\proglang{R} programming language \citep{rcore2024}. GLMs add an
additional layer of complexity compared to linear regression models, as
the left-hand size of Equation\(~\)\ref{eq-glm} is a function of the
mean of \(\mathbf{y}\) rather than \(\mathbf{y}\) itself (as in linear
regression models).

The standard GLM assumes the elements of \(\mathbf{y}\) are independent.
This independence assumption is typically impractical for spatial data.
In spatial data, nearby observations tend to be more similar than
distant observations \citep{tobler1970computer}, leading to positive
spatial covariance among observations. The consequences of ignoring
spatial covariance in statistical models for spatial data can be severe
and include imprecise parameter estimates as well as misleading standard
errors that inflate Type-I error rates and decrease power
\citep{zimmerman2024spatial}.

An approach for handling spatial data using a GLM is to assume
\(\mathbf{w}\) has spatial covariance. This is achieved by adding to
Equation\(~\)\ref{eq-glm} two random effects, \(\boldsymbol{\tau}\) and
\(\boldsymbol{\epsilon}\). The random effect \(\boldsymbol{\tau}\) is an
\(n \times 1\) column vector of spatially dependent random errors. We
assume that \(\text{E}(\boldsymbol{\tau}) = \boldsymbol{0}\) and
\(\text{Cov}(\boldsymbol{\tau}) = \sigma^2_\tau \mathbf{R}\), where
\(\text{E}(\cdot)\) and \(\text{Cov}(\cdot)\) denote expectation and
covariance, respectively. The variance parameter \(\sigma^2_\tau\)
controls the magnitude of spatial covariance and is often called a
partial sill, while the matrix \(\mathbf{R}\) is an \(n \times n\)
spatial correlation matrix that depends on a range parameter controls
the distance-decay rate of the spatial correlation. One example of a
spatial covariance matrix is the ``exponential'', which is given by
\begin{equation}\label{eq-spcov-exp}
  \text{Cov}(\boldsymbol{\tau}) = \sigma^2_{de} \exp(-\mathbf{H}/\phi),
\end{equation} where \(\mathbf{H}\) is a matrix of pairwise distances
among the elements of \(\mathbf{y}\) and \(\phi\) is a range parameter.
From Equation\(~\)\ref{eq-spcov-exp}, as the distance between two
elements of \(\mathbf{y}\) increases, the spatial covariance decreases,
which reflects intuition. Moreover, as the range parameter, \(\phi\),
increases, the strength of spatial dependence increases
(Figure\(~\)\ref{fig-range}). The random effect
\(\boldsymbol{\epsilon}\) is an \(n \times 1\) column vector of
independent random errors. We assume that
\(\text{E}(\boldsymbol{\epsilon}) = \boldsymbol{0}\) and
\(\text{Cov}(\boldsymbol{\tau}) = \sigma^2_\epsilon \mathbf{I}\), where
\(\mathbf{I}\) is an \(n \times n\) identity matrix. The variance
parameter \(\sigma^2_\epsilon\) controls the magnitude of nonspatial
variability (i.e., fine-scale variation) and is often called a nugget.

\begin{figure}
\centering
\includegraphics[width = 0.7\linewidth]{figures/figure-01.png}
\caption{An exponential spatial correlation function with varying range parameters.}
\label{fig-range}
\end{figure}

Through inclusion of \(\boldsymbol{\tau}\) and
\(\boldsymbol{\epsilon}\), the spatial GLM (SPGLM) can be written as
\begin{equation}\label{eq-spglm}
f(\boldsymbol{\mu}|\mathbf{X}, \boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{\epsilon}) \equiv \mathbf{w} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon}.
\end{equation} Often in spatial statistics, quantities are explicitly
referenced with respect to \(\mathbf{s}\), a vector of coordinates
indexing the observation \citep{cressie1993statistics}. For example,
\(\mathbf{y}\) and \(\mathbf{X}\) may instead be written
\(\mathbf{y}(\mathbf{s})\) and \(\mathbf{X}\), respectively. We
acknowledge the utility of this nomenclature but drop the explicit
dependence on \(\mathbf{s}\) for simplicity of notation. Assuming
independence among \(\boldsymbol{\tau}\) and \(\boldsymbol{\epsilon}\),
it follows that \begin{equation}\label{eq-spcov}
 \text{Cov}(\boldsymbol{\tau} + \boldsymbol{\epsilon}) = \text{Cov}(\boldsymbol{\tau}) + \text{Cov}(\boldsymbol{\epsilon}) = \sigma^2_{\tau}\mathbf{R} + \sigma^2_{\epsilon} \mathbf{I}.
\end{equation} To better align with intuition, we henceforth
\(\sigma^2_{\tau}\) as \(\sigma^2_{de}\) (for spatial error variance)
and \(\sigma^2_{\epsilon}\) as \(\sigma^2_{ie}\) (for independent error
variance). The parameters \(\sigma^2_{de}\), \(\sigma^2_{ie}\), the
range parameter \(\phi\) in \(\mathbf{R}\), and any other parameters in
\(\mathbf{R}\) compose \(\boldsymbol{\theta}\), the covariance parameter
vector.

Fitting and using SPGLMs is challenging both conceptually and
computationally \citep{bolker2009generalized}. Recently, however, there
have been numerous, significant advances in \proglang{R} software that
have made these models more accessible to practitioners. The \pkg{brms}
\citep{burkner2017brms}, \pkg{carBayes} \citep{lee2013carbayes},
\pkg{ngspatial} \citep{hughes2020ngspatial}, \pkg{R-INLA}
\citep{lindgren2015bayesian} and \pkg{inlabru} \citep{bachl2019inlabru},
\pkg{spBayes} \citep{finley2007spbayes}, \pkg{spOccupancy}
\citep{doser2022spoccupancy}, \pkg{spAbundance}
\citep{doser2024spabundance}, and \pkg{spNNGP} \citep{finley2002spnngp}
packages take a Bayesian approach, either directly sampling from
posterior distributions of parameters (e.g., using MCMC) or
approximating them. A benefit of Bayesian approaches is that prior
information can be incorporated and uncertainty quantification of
parameter estimates is straightforward. However, Bayesian approaches,
especially those using MCMC, can be computationally expensive. In order
to reduce computation time, many of these packages work with the
precision matrix instead of the covariance matrix so that
computationally expensive matrix inversion is not required. For example,
\pkg{R-INLA} uses the precision matrix and tends to be very fast.
Working with precision matrices, however, can be more restrictive and
less intuitive than working directly with the covariance matrix. The
{FRK} \citep{sainsbury2024modeling}, \pkg{glmmTMB}
\citep{brooks2017glmmtmb}, \pkg{hglm} \citep{ronnegard2010hglm},
\pkg{mgcv} \citep{wood2017generalized}, and \pkg{spaMM}
\citep{rousset2014spamm} packages directly use Laplace,
quasi-likelihood, or reduced-rank approaches to estimate parameters.
These direct approaches tend to be computationally efficient, as they
don't rely on MCMC sampling. In contrast to the Bayesian approach, a
drawback of these direct approaches is that prior information cannot be
formally incorporated and covariance parameter uncertainty is more
challenging to quantify. The \pkg{sdmTMB} \citep{anderson2024sdmtmb}
package combines elements of \pkg{R-INLA}, \pkg{glmmTMB}, and properties
of Gaussian Markov random fields to fit a wide variety of SPGLMs, and
\pkg{tinyVAST} \citep{thorson2025tinyVAST} extends some of these models
to multivariate or (dynamic) structural equation models.

\citet{ver2024marginal} proposed a novel approach to fitting SPGLMs that
leverages the Laplace approximation while marginalizing over both the
latent \(\mathbf{w}\) and the fixed effects (\(\boldsymbol{\beta}\)) and
accommodating spatial covariance. \citet{ver2024marginal} showed that
this approach performed efficiently in a variety of simulation settings,
generally having appropriate confidence interval coverage for the fixed
effects and prediction interval coverage for new \(\mathbf{w}\). The
approach performed similarly to the Bayesian SPGLM approach in
\pkg{spBayes} and the automatic differentiation SPGLM approach in
\pkg{glmmTMB} but was much faster. At small sample sizes, the approach
outperformed the approximate Bayesian SPGLM approach in \pkg{R-INLA} and
had similar computational times. For moderate sample sizes, it performed
similarly to \pkg{R-INLA}, though \pkg{R-INLA} was faster. This novel
approach is particularly attractive for two reasons. First, it is
general enough that can be applied to any covariance structure (not just
spatial). Second, after estimating the covariance parameters, analytical
solutions exist for the fixed effects (and their standard errors) as
well as predictions of the latent \(\mathbf{w}\) at new locations (and
their standard errors). The \pkg{spmodel} \proglang{R} package
\citep{dumelle2023spmodel} recently provided full support for the
methods in \citet{ver2024marginal} applied to binary, count, skewed, and
proportion data for over 20 different spatial covariance types.

The \pkg{spmodel} \proglang{R} package \citep{dumelle2023spmodel}
recently provided a full set of modeling tools for SPGLMs fit using the
methods described in \citet{ver2024marginal}. These modeling tools are
approachable and mirror the familiar \code{glm()} syntax from
base-\proglang{R}, making the transition from GLMs to SPGLMs relatively
seamless. The \code{spglm()} function fits SPGLMs for point-referenced
data (e.g., x-coordinates and y-coordinates representing point locations
in a field), while the \code{spgautor()} function fits SPGLMs for areal
data (e.g., polygon boundaries representing geographic subsets of a
region). \pkg{spmodel} supports the binomial distribution for binary
data, Poisson and negative binomial distributions for count data, Gamma
and inverse Gaussian distributions for skewed data, and the beta
distribution for proportion data. There are 20 different spatial
covariance structures available including the exponential, Gaussian, and
spherical for point-referenced data (Figure\(~\)\ref{fig-type}) and the
conditional autoregressive, and simultaneous autoregressive structures
for areal data. \pkg{spmodel} provides tools for commonly used model
summaries, visualizations, and diagnostics (e.g., Cook's distance) using
standard \proglang{R} helper functions like \code{summary()},
\code{plot()}, and \code{cooks.distance()}. \pkg{spmodel} also provides
tools to predict \(\mathbf{w}\) at new locations and quantify
uncertainty in those prediction using \code{predict()}. This core
functionality, combined with several advanced features we describe
throughout the manuscript, enable \pkg{spmodel} to provide some novel
and important capabilities previously missing from the existing SPGLM
ecosystem in \proglang{R}.

\begin{figure}
\centering
\includegraphics[width = 0.7\linewidth]{figures/figure-02.png}
\caption{Exponential, Gaussian, and spherical spatial correlation functions all with range parameters equal to 0.5.}
\label{fig-type}
\end{figure}

\pkg{spmodel} (version 0.11.0) is arguably most similar to \pkg{sdmTMB}
(version 0.7.4) in terms of scope and feel. Both packages use similar
syntax as \code{glm()}, accommodate flexible \code{formula} arguments
(e.g., offsets, splines), handle spatial covariance that decays at
different rates in different rates (i.e., geometric anisotropy),
incorporate nonspatial random effects, support other \proglang{R}
packages for modeling like \pkg{broom}
\citep{robinson2021broom, kuhn2022tidy}, \pkg{emmeans}
\citep{lenth2024emmeans}, and \pkg{car} \citep{fox2019car}, and have
tools for model summaries, prediction, and simulating data. There are
some notable differences between the two packages, however. \pkg{sdmTMB}
supports several additional GLM distributions like the Tweedie, supports
Hurdle models, and can incorporate prior information through Bayesian
applications. \pkg{sdmTMB} also provides tools for working with temporal
data and enhanced visualizations of marginal effects. \pkg{sdmTMB} does
require a preprocessing step of constructing a mesh for the stochastic
partial differential equation approach, and the density of the mesh can
affect model results and computational complexity. \pkg{spmodel} does
not require the construction of a mesh prior to modeling. \pkg{spmodel}
supports 20 different spatial covariances and models them directly,
rather than using a precision matrix approximation to the MatÃ©rn spatial
covariance as in \pkg{sdmTMB}. \pkg{spmodel} also provides experimental
design tools (e.g., analysis of variance, contrasts), supports \pkg{sf}
objects in modeling and prediction functions \citep{pebesma2018sf}, has
several specialized model diagnostics like leverage values and Cook's
distances, and has analytic solutions for prediction standard errors.
Other similarities and differences do exist between \pkg{sdmTMB} and
\pkg{spmodel}, and both packages continue to evolve. Overall, we believe
that these packages are complementary and enhance the suite of SPGLM
tools accessible to practitioners.

The rest of this article is organized as follows. In
Section\(~\)\ref{sec-spglm}, we provide some background for the SPGLM
fitting and prediction routines in \pkg{spmodel}. In
Section\(~\)\ref{sec-applications}, we provide several applications of
\pkg{spmodel} to spatial binary, count, skewed and proportion data with
both point-referenced and areal supports. And in
Section\(~\)\ref{sec-discussion}, we end with a discussion synthesizing
\pkg{spmodel}'s contributions to the analysis of SPGLMs in \proglang{R}.

\section{The spatial generalized linear model and
marginalizatoin}\label{sec-spglm}

\pkg{spmodel} implements the novel methods described in
\citet{ver2024marginal} to fit SPGLMs, which leverages the Laplace
approximation and marginalizes over both the latent \(\mathbf{w}\) and
the fixed effects while accommodating spatial covariance. A beneficial
aspect of this approach is that it formally maximizes a hierarchical GLM
likelihood \citep{lee1996hierarchical, wood2017generalized}. This makes
likelihood-based statistics for model comparison like AIC
\citep{akaike1974new}, AICc \citep{hoeting2006model}, BIC
\citep{schwarz1978estimating}, deviance
\citep{mccullagh1989generalized}, and likelihood ratio tests available.
These types of statistics are not available for quasi-likelihood
\citep{wedderburn1974quasi, breslow1993approximate} or pseudo-likelihood
approaches \citep{wolfinger1993generalized}, which only specify the
first two moments of a distribution. \citet{ver2024marginal} provides
thorough details regarding the method and contextualizes its development
which built upon similar methods
\citep[\citet{bonat2016practical}]{evangelou2011estimation}. Next, we
describe a brief overview of the approach and how it can be used for
parameter estimation, inference, and prediction.

\subsection{Formulating the hierarchical
likelihood}\label{formulating-the-hierarchical-likelihood}

We can write the SPGLM likelihood hierarchically as
\begin{equation}\label{eq-marginal}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} \int_{\boldsymbol{\beta}} [\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] d\boldsymbol{\beta} d\mathbf{w},
\end{equation} where \([\mathbf{y} | f^{-1}(\mathbf{w}), \varphi]\) is
the density for the appropriate response distribution of \(\mathbf{y}\)
(e.g., binomial, Poisson) given the latent \(\mathbf{w}\) and dispersion
parameter (\(\varphi\)), and
\([\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}]\) is the multivariate
Gaussian density for \(\mathbf{w}\) given the explanatory variables
(\(\mathbf{X}\)), fixed effects (\(\boldsymbol{\beta}\)), and spatial
covariance parameters (\(\boldsymbol{\theta}\)). The elements of
\([\mathbf{y} | f^{-1}(\mathbf{w}), \varphi]\) are conditionally
independent (given \(\mathbf{w}\)), but the elements of
\([\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}]\) share spatial
covariance. Following \citet{harville1977maximum}, we can integrate
\(\boldsymbol{\beta}\) out of Equation\(~\)\ref{eq-spglm}, which yields
\begin{equation}\label{eq-marginal2}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} [\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] d\mathbf{w},
\end{equation} where \([\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}]\)
is the restricted (i.e., residual) multivariate Gaussian density
\citep{patterson1971recovery} for \(\mathbf{w}\) given the explanatory
variables and covariance parameters. Equation\(~\)\ref{eq-marginal2} can
be synonymous written after profiling the overall variance out of
\(\boldsymbol{\Sigma}\), which reduces the dimension of
\(\boldsymbol{\theta}\) by one for optimization
\citep{wolfinger1994computing}. The restricted multivariate Gaussian
density is given by \begin{equation}\label{eq-reml-def}
[\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}] = \frac{\exp(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\tilde{\boldsymbol{\beta}}) \boldsymbol{\Sigma}^{-1} (\mathbf{y} - \mathbf{X}\tilde{\boldsymbol{\beta}})^T)}{(2 \pi)^{(n - p)/2} |\boldsymbol{\Sigma}|^{1/2}|\mathbf{X}^T \boldsymbol{\Sigma}^{-1} \mathbf{X}|^{1/2}},
\end{equation} where
\(\tilde{\boldsymbol{\beta}} = (\mathbf{X}^T \boldsymbol{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\Sigma}^{-1} \mathbf{w}\)
and \(|\cdot|\) denotes the determinant. Next, let
\begin{equation}\label{eq-marginal03}
  \ell_\mathbf{w} = \log([\mathbf{y} | f^{-1}(\mathbf{w}), \varphi] [\mathbf{w} | \mathbf{X}, \boldsymbol{\theta}])
\end{equation} and rewrite Equation\(~\)\ref{eq-marginal2} as
\begin{equation}\label{eq-marginal3}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] = \int_{\mathbf{w}} \exp(\ell_\mathbf{w}) d\mathbf{w}.
\end{equation} A second-order Taylor series expansion of
\(\ell_\mathbf{w}\) around \(\hat{\mathbf{w}}\) yields
\begin{equation}\label{eq-marginal4}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx \int_{\mathbf{w}} \exp(\ell_{\hat{\mathbf{w}}} + \mathbf{g}^T(\mathbf{w} - \hat{\mathbf{w}}) + \frac{1}{2}(\mathbf{w} - \hat{\mathbf{w}})^T \mathbf{G} (\mathbf{w} - \hat{\mathbf{w}}))d\mathbf{w},
\end{equation} where \(\mathbf{g}\) and \(\mathbf{G}\) are the gradient
and Hessian, respectively, of \(\ell_\mathbf{w}\) with respect to
\(\mathbf{w}\). If \(\hat{\mathbf{w}}\) is a value for which
\(\mathbf{g} = \mathbf{0}\), \begin{equation}\label{eq-marginal5}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx \exp(\ell_{\hat{\mathbf{w}}}) \int_{\mathbf{w}} \exp(-\frac{1}{2}(\mathbf{w} - \hat{\mathbf{w}})^T (-\mathbf{G}) (\mathbf{w} - \hat{\mathbf{w}}))d\mathbf{w}.
\end{equation} The integral in Equation\(~\)\ref{eq-marginal5} can be
solved by leveraging properties of the normalizing constant of a
multivariate Gaussian distribution. Thus, rewriting
\(\exp(\ell_{\hat{\mathbf{w}}})\) yields
\begin{equation}\label{eq-marginal6}
  [\mathbf{y}|\mathbf{X}, \varphi, \boldsymbol{\theta}] \approx [\mathbf{y} | f^{-1}(\hat{\mathbf{w}}), \varphi] [\hat{\mathbf{w}} | \mathbf{X}, \boldsymbol{\theta}] (2 \pi)^{n/2}|-\mathbf{G}_{\hat{\mathbf{w}}}|^{-1/2}.
\end{equation}

Maximizing the natural logarithm of Equation\(~\)\ref{eq-marginal6}
requires a doubly iterative process over \(\boldsymbol{\theta}\) and
\(\varphi\) as well as \(\mathbf{w}\), eventually yielding the the
marginal restricted maximum likelihood estimators \(\hat{\varphi}\) and
\(\hat{\boldsymbol{\theta}}\) and their corresponding values of
\(\hat{\mathbf{w}}\). Maximizing this log likelihood is a
computationally expensive operation that involves repeatedly evaluating
\(\boldsymbol{\Sigma}^{-1}\), \(\mathbf{g}\), and \(\mathbf{G}\); see
\citet{ver2024marginal} for more details and forms of \(\mathbf{g}\) and
\(\mathbf{G}\) for various response distributions.

\subsection{Estimating fixed effects}\label{estimating-fixed-effects}

Though the fixed effects are integrated out of the likelihood, we can
still estimate them using generalized least squares (GLS) principles, a
common practice for linear models estimated using restricted maximum
likelihood methods. Had we observed \(\mathbf{w}\), a GLS estimator for
\(\boldsymbol{\beta}\) is given by \begin{equation}\label{eq-gls1}
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{w} = \mathbf{B}\mathbf{w},
\end{equation} where
\(\mathbf{B} = (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\).
However, we only observe \(\hat{\mathbf{w}}\), so it is reasonable to
define \(\hat{\boldsymbol{\beta}} = \mathbf{B}\hat{\mathbf{w}}\). Thus,
to derive properties of \(\hat{\boldsymbol{\beta}}\) like expectation
and variance, we must derive these properties for \(\hat{\mathbf{w}}\).
To do so, we must condition on \(\mathbf{w}\) as if it were observed and
invoke properties of the laws of total expectation and variance. Because
\(\hat{\mathbf{w}}\) was optimized via the likelihood, we assume that
given \(\mathbf{w}\), \(\hat{\mathbf{w}}\) has mean \(\mathbf{w}\) and
variance approximately equal to \(-\mathbf{H}^{-1}\) (the inverse
Hessian). It follows that \(\text{E}(\hat{\mathbf{w}})\) is given by
\begin{equation}
  \text{E}(\hat{\mathbf{w}}) = \text{E}(\text{E}(\hat{\mathbf{w}} | \mathbf{w}))
   = \text{E}(\mathbf{w}) 
   = \mathbf{X}\boldsymbol{\beta}
\end{equation} and \(\text{Var}(\hat{\mathbf{w}})\) is given by
\begin{align}
  \text{Var}(\hat{\mathbf{w}}) & = \text{E}(\text{Var}(\hat{\mathbf{w}} | \mathbf{w})) + \text{Var}(\text{E}(\hat{\mathbf{w}} | \mathbf{w})) \\
  & = \text{E}(-\mathbf{H}^{-1}) + \text{Var}(\mathbf{w})\\
  & = -\mathbf{H}^{-1} + \boldsymbol{\Sigma}
\end{align} Putting this all together, it follows that \begin{equation}
  \text{E}(\hat{\boldsymbol{\beta}})  = \text{E}(\mathbf{B}\hat{\mathbf{w}}) 
  = \mathbf{B}\text{E}(\hat{\mathbf{w}}) =  (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1} \mathbf{X})\boldsymbol{\beta} = \boldsymbol{\beta}
\end{equation} and \begin{align}
  \text{Var}(\hat{\boldsymbol{\beta}}) & = \text{Var}(\mathbf{B}\hat{\mathbf{w}}) \\
  & = \mathbf{B} \text{Var}(\hat{\mathbf{w}}) \mathbf{B}^\top\\
  & = \mathbf{B} (-\mathbf{H}^{-1} + \boldsymbol{\Sigma}) \mathbf{B}^\top \\
  & = \mathbf{B}-\mathbf{H}^{-1}\mathbf{B}^\top + \mathbf{B}\boldsymbol{\Sigma}\mathbf{B}^\top \\
  & = \mathbf{B}-\mathbf{H}^{-1}\mathbf{B}^\top + (\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}
\end{align} In practice, \(\text{Var}(\hat{\boldsymbol{\beta}})\) is
estimated by evaluating \(\boldsymbol{\Sigma}\) at
\(\hat{\boldsymbol{\theta}}\), the estimated covariance parameter
vector.

These results are important because they justify closed-form solutions
for \(\hat{\boldsymbol{\beta}}\) and its associated variance.
Closed-form solutions are useful because they bypass the need for
computationally expensive sampling-based strategies to evaluate the mean
and variance of \(\hat{\boldsymbol{\beta}}\) -- a common technique for
other approaches to SPGLMs like Bayesian MCMC.

\subsection{Inspecting model
diagnostics}\label{inspecting-model-diagnostics}

Inspecting model diagnostics is an important step of the modeling
process that can yield valuable insights into model behavior and unusual
observations. \citet{montgomery2021introduction} contextualize three
components of unusual observations: outliers, leverage, and influence.
An observation is an outlier if it has an unusual response value
relative to expectation. The response GLM residuals simply compare the
observation to its fitted latent mean: \begin{equation}
  \mathbf{r}_{r} = \mathbf{y} - f^{-1}(\hat{\mathbf{w}})
\end{equation} Because observations often have a unique support in a GLM
(e.g., only two possible response values for binary data) and the
variance of an observation generally depends on its mean, response
residuals lack some utility. Deviance residuals are a function of
response residuals that are appropriately scaled to behave more like
response residuals in a standard linear model. Deviance residuals are
given by \begin{equation}
  \mathbf{r}_{d} = sign(\mathbf{r}_{r})\sqrt{\mathbf{d}},
\end{equation} where \(\mathbf{d}\) is a vector of individual deviances.
The sum of the squared deviance residuals equals the sum of
\(\mathbf{d}\). The sum of \(\mathbf{d}\) is the deviance of the model
fit, which quantifies twice the difference in log likelihoods between
the a saturated model that fits every observation perfectly (i.e.,
\(\mathbf{y} = f^{-1}(\hat{\mathbf{w}}_i)\) for all \(i\)) and the
fitted model. Deviance is often used as a fit statistic; lower values of
deviance imply a better model fit. Pearson and standardized residuals
are other types of GLM residuals that involve some scaling of the
response residuals; the Pearson residuals scale \(\mathbf{r}_{r}\) by
the squrae root of \(\mathbf{V}\), while the standardized residuals
scale the deviance residuals by
\(\frac{1}{\sqrt{(1 - \mathbf{L}_{ii})}}\), where \(\mathbf{L}_{ii}\) is
the \(i\)th diagonal element of the leverage matrix, which we discuss
next. An observation has high leverage if its combination of explanatory
variables is far away from other observations. In a linear model, the
leverage values are the diagonal of the leverage (i.e., projection, hat)
matrix,
\(\mathbf{L} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\).
In a GLM, the leverage matrix is given by \begin{equation}
  \mathbf{L} = \mathbf{V}^{1/2} \mathbf{X} (\mathbf{X}^\top \mathbf{V} \mathbf{X}) \mathbf{X}^\top \mathbf{V}^{1/2},
\end{equation} where \(\mathbf{V}\) is a diagonal matrix with \(i\)th
diagonal element equal to the variance of the response distribution
evaluated at \(f^{-1}(\mathbf{w}_i)\) \citep{faraway2016extending}. The
matrix \(\mathbf{V}\) is sometimes called the GLM weight matrix. The
larger the \(i\)th diagonal element of the hat matrix, the more severe
the leverage from the \(i\)th observation. An observation is influential
if it has a sizeable impact on model fit. Influence is measured using
Cook's distance \citep{cook1979influential, cook1982residuals}, which is
given for a GLM by \begin{equation}
  \mathbf{c} = \mathbf{r}^2_{s} \frac{diag(\mathbf{L})}{\text{tr}(\mathbf{L})(\mathbf{1} - diag(\mathbf{L}))},
\end{equation} where \(\mathbf{r}^2_{s}\) are the standardized residuals
and \(diag(\mathbf{L})\) indicates the diagonal elements of the leverage
matrix. The larger the \(i\)th diagonal element of the hat matrix, the
more severe the influence from the \(i\)th observation.
\citet{montgomery2021introduction} provide guidance for interpreting
these types of statistics, including cutoffs to consider when
identifying unusual residual, leverage, or influence values.

In a linear model, the \(R^2\) (R-squared) statistic quantifies the
proportion of variability in the data captured by the explanatory
variables and is calculated as one minus the ratio of the error sum of
squares to the total sum of squares \citep{rencher2008linear}. In a GLM,
there are many ways to define such a statistic
\citep{smith2013comparison}. One such approach is to use one minus the
deviance ratio: \begin{equation}
  PR^2 = 1 - \frac{deviance_{fit}}{deviance_{null}},
\end{equation} where \(deviance_{fit}\) is the deviance of the fitted
model (sometimes called the residual deviance) and \(deviance_{null}\)
is the deviance of the model taking \(\mathbf{X} \equiv \mathbf{1}\), a
column of all ones (i.e., an intercept-only model). In practice,
\(deviance_{null}\) is derived by computing \(\hat{\mathbf{w}}\) when
\(\mathbf{X} \equiv \mathbf{1}\) given \(\hat{\boldsymbol{\theta}}\) and
\(\hat{\varphi}\) from the fitted model. Like the standard \(R^2\), this
statistic attempts to capture variability (i.e., deviance) attributable
to the explanatory variables. Because the \(deviance_{null}\)
denominator changes across fitted models (as the values of
\(\hat{\boldsymbol{\theta}}\) and \(\hat{\varphi}\) change), this
statistic should not be used as a model comparison tool. Instead, it
should be used as an informative diagnostic tool unique to each model
fit.

\subsection{Predicting at new
locations}\label{predicting-at-new-locations}

We may also predict values of the latent mean (on the link scale) at new
locations by leveraging the spatial covariance between observed
locations and new locations (spatial prediction is also called Kriging;
see \citet{cressie1990origins}). Again suppose that we observed
\(\mathbf{w}\) and we want to make predictions at \(\mathbf{u}\), a
vector of latent means at the new locations that follows the same SPGLM
from Equation\textasciitilde{}\ref{eq-spglm} with fixed effects design
matrix, \(\mathbf{X}_{\mathbf{u}}\). The vector
\((\mathbf{w}, \mathbf{u})^\top\) has the following properties:
\begin{align}
  \text{E}(\mathbf{w}, \mathbf{u})^\top & = (\text{E}(\mathbf{w}), \text{E}(\mathbf{u}))^\top = (\mathbf{X}\boldsymbol{\beta}, \mathbf{X}_\mathbf{u}\boldsymbol{\beta})^\top \\
  \text{Var}(\mathbf{w}, \mathbf{u})^\top & = \begin{bmatrix} \text{Var}(\mathbf{w}, \mathbf{w}) & \text{Var}(\mathbf{w}, \mathbf{u}) \\ \text{Var}(\mathbf{u}, \mathbf{w}) & \text{Var}(\mathbf{u}, \mathbf{u}) \end{bmatrix} = \begin{bmatrix} \boldsymbol{\Sigma} & \boldsymbol{\Sigma}_{\mathbf{w}\mathbf{u}} \\ \boldsymbol{\Sigma}_{\mathbf{u}\mathbf{w}} & \boldsymbol{\Sigma}_{\mathbf{u}\mathbf{u}} \end{bmatrix}
\end{align} Because we have observed \(\mathbf{w}\), we may derive the
conditional distribution of \(\mathbf{u}|\mathbf{w}\), which has the
following properties: \begin{align}
  \text{E}(\mathbf{w} | \mathbf{u}) & = \mathbf{X}_{\mathbf{u}} \boldsymbol{\beta} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\mathbf{w} - \mathbf{X}\boldsymbol{\beta}) \\
  \text{E}(\mathbf{w} | \mathbf{u}) & = \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}_{\mathbf{w}, \mathbf{u}}
\end{align} \citet{ver2024marginal} show how these equations are
adjusted to reflect uncertainty in both \(\hat{\boldsymbol{\beta}}\) and
\(\hat{\mathbf{w}}\) while leveraging the laws of total expectation and
variance yet again. They derive the predictor of \(\mathbf{u}\),
\(\hat{\mathbf{u}}\), and its associated variance, given by:
\begin{align}
  \hat{\mathbf{u}} & = \mathbf{X}_{\mathbf{u}} \hat{\boldsymbol{\beta}} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\hat{\mathbf{w}} - \mathbf{X}\hat{\boldsymbol{\beta}}) \\
  \text{Var}(\hat{\mathbf{u}}) & = \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}_{\mathbf{w}, \mathbf{u}} + \mathbf{K}(\mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{K}^\top + \boldsymbol{\Lambda}(-\mathbf{H})^{-1}\boldsymbol{\Lambda}^\top,
\end{align} where
\(\mathbf{K} = \mathbf{X}_{\mathbf{u}} - \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}} \boldsymbol{\Sigma}^{-1} \mathbf{X}\)
and
\(\boldsymbol{\Lambda} = \mathbf{X}_{\mathbf{u}}\mathbf{B} + \boldsymbol{\Sigma}_{\mathbf{u}, \mathbf{w}}\boldsymbol{\Sigma}^{-1}(\mathbf{1} - \mathbf{X}\mathbf{B})\)
for a vector of ones, \(\mathbf{1}\).

As with \(\hat{\boldsymbol{\beta}}\), in practice these covariance
matrices are evaluated at \(\hat{\boldsymbol{\theta}}\). Moreover, these
closed-form solutions provided enhance computational efficiency and
clarity of the predictor's behavior.

\section{Modeling moose presence in Alaska, USA}\label{sec-applications}

The \code{moose} data in \pkg{spmodel} contain information on moose
(Alces Alces) presence in the Togiak region of Alaska, USA.
\code{moose}is an \code{sf} object, a special data frame that is
supplemented with spatial information using the \pkg{sf} package in
\proglang{R} \citep{pebesma2018sf}. The first few rows of \code{moose}
look like:

\begin{CodeChunk}
\begin{CodeInput}
R> head(moose)
\end{CodeInput}
\begin{CodeOutput}
Simple feature collection with 6 features and 4 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 281896.4 ymin: 1518398 xmax: 311325.3 ymax: 1541016
Projected CRS: NAD83 / Alaska Albers
# A tibble: 6 x 5
   elev strat count presence           geometry
  <dbl> <chr> <dbl> <fct>           <POINT [m]>
1  469. L         0 0        (293542.6 1541016)
2  362. L         0 0        (298313.1 1533972)
3  173. M         0 0        (281896.4 1532516)
4  280. L         0 0        (298651.3 1530264)
5  620. L         0 0        (311325.3 1527705)
6  164. M         0 0        (291421.5 1518398)
\end{CodeOutput}
\end{CodeChunk}

There are five columns: \code{elev}, the numeric site elevation
(meters); \code{strat} a stratification variable for sampling with two
levels, \code{"L"} and \code{"M"}, which are categorized by landscape
metrics at each site; \code{count}, the number of moose at each site;
\code{presence}, a factor that indicates whether at least one moose was
observed at each site (\code{0} implies no moose; \code{1} implies at
least one moose); and \code{geometry}, the NAD83/Alaska Albers (EPSG:
3338) projected coordinate of each site (these data are point-referenced
because each observation occurs at point coordinates and are represented
by a \code{POINT} geometry. The \code{moose_preds} data in \pkg{spmodel}
contain spatial locations at which predictions of moose presence are
desired (and is also point-referenced). \code{moose_preds} is also an
\code{sf} object with measurements for \code{elev} and \code{strat} and
the same projection system. Figure\(~\)\ref{fig-moose-data} shows the
\code{presence} variable in \code{moose} as well as the spatial
locations of both \code{moose} and \code{moose_preds}. Moose are most
commonly present in the southwestern and eastern parts of the domain and
least commonly present in the northwest
(Figure\(~\)\ref{fig-moose-data}). Next we show how to use \pkg{spmodel}
to study the effect of elevation and strata on moose presence while
accounting for spatial covariance and to make predictions of moose
presence at new locations.

\begin{figure}
\centering
\includegraphics[width = 0.70\linewidth]{figures/figure-1.png}
\caption{Moose presence in Alaska. Circles represent moose presence or absence (based on color) and triangles represent locations at which moose presence probability predictions are desired.}
\label{fig-moose-data}
\end{figure}

\subsection{Model Fitting}\label{model-fitting}

SPGLMs in \pkg{spmodel} are fit using the \code{spglm()} function. The
\code{spglm()} function requires four arguments: \texttt{formula}, the
relationship between the response and explanatory variables;
\texttt{family}, the response distribution assumed for the repsonse
variable; \texttt{data}, the data frame that contains the variables in
\texttt{formula}, and \texttt{spcov\_type}, the type of spatial
covariance. These first three arguments are the three required arguments
to \code{glm()} for nonspatial GLMs. So, the transition from
\code{glm()} to \code{spglm()} simply requires one additional argument:
\code{spcov_type}. When \code{data} is not an \code{sf} object,
\code{spglm()} also requires the \code{xcoord} and \code{ycoord}
arguments, which indicate the columns in \code{data} that represent the
x- and y-coordinates, respectively (it is assumed these coordinates are
already projected).

We use \code{spglm()} to fit a spatial logistic regression model
quantifying the effect of elevation and strata on moose presence:

\begin{CodeChunk}
\begin{CodeInput}
R> spbin <- spglm(
+   formula = presence ~ elev + strat,
+   family = binomial,
+   data = moose,
+   spcov_type = "exponential"
+ )
\end{CodeInput}
\end{CodeChunk}

The \code{summary()} function returns a model summary that returns
relevant information like the function call, deviance residuals, a
coefficients table of fixed effects, the pseudo R-squared, spatial
covariance parameter coefficient estimates, and the GLM dispersion
parameter (fixed at one in logistic regression):

\begin{CodeChunk}
\begin{CodeInput}
R> summary(spbin)
\end{CodeInput}
\begin{CodeOutput}

Call:
spglm(formula = presence ~ elev + strat, family = binomial, data = moose, 
    spcov_type = "exponential")

Deviance Residuals:
    Min      1Q  Median      3Q     Max 
-1.7535 -0.8005  0.3484  0.7893  1.5797 

Coefficients (fixed):
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -2.465713   1.486212  -1.659 0.097104 .  
elev         0.006036   0.003525   1.712 0.086861 .  
stratM       1.439273   0.420591   3.422 0.000622 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared: 0.06275

Coefficients (exponential spatial covariance):
       de        ie     range 
5.145e+00 1.294e-03 4.199e+04 

Coefficients (Dispersion for binomial family):
dispersion 
         1 
\end{CodeOutput}
\end{CodeChunk}

Based on this model, there is some evidence that elevation is associated
with higher probabilities of moose presence (\(p\)-value \(\approx\)
0.087) but strong evidence that moose are more prevalent in the
\code{"M"} strata than the \code{"L"} strata (\(p\)-value \textless{}
0.001). The fixed effects coefficients table from \code{summary()} is
often of primary practical interest, but it is not easily usable when
printed directly to the \proglang{R} console. The \code{tidy()} function
tidies this table, turning it into a data frame (i.e., a tibble) with
standard column names:

\begin{CodeChunk}
\begin{CodeInput}
R> tidy(spbin, conf.int = TRUE)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 3 x 7
  term        estimate std.error statistic  p.value  conf.low conf.high
  <chr>          <dbl>     <dbl>     <dbl>    <dbl>     <dbl>     <dbl>
1 (Intercept) -2.47      1.49        -1.66 0.0971   -5.38        0.447 
2 elev         0.00604   0.00353      1.71 0.0869   -0.000873    0.0129
3 stratM       1.44      0.421        3.42 0.000622  0.615       2.26  
\end{CodeOutput}
\end{CodeChunk}

\subsection{Model Comparison}\label{model-comparison}

The strength of spatial covarinace in the data affects how beneficial a
SPGLM is relative to a GLM. When the spatial covariance is strong, the
SPGLM should notably outperform the GLM. When the spatial covariance is
weak, the SPGLM and GLM should perform similarly. We can quantify the
benefits of incorporating spatial covariance for a particular data set
by comparing the fit of a SPGLM to a GLM. We can fit a GLM in
\code{spmodel} by specifying \code{spcov_type = "none"}:

\begin{CodeChunk}
\begin{CodeInput}
R> bin <- spglm(
+   formula = presence ~ elev + strat,
+   family = binomial,
+   data = moose,
+   spcov_type = "none"
+ )
\end{CodeInput}
\end{CodeChunk}

While the \code{spglm()} approach evaluates the HGLMM likelhood with
\(\sigma^2_{de} = 0\) and \(\sigma^2_{ie} \approx 0\) instead of just
the GLM likelihood, the parameter estimates and their standard errors
are the same:

\begin{CodeChunk}
\begin{CodeInput}
R> bin_glm <- glm(
+   formula = presence ~ elev + strat,
+   family = binomial,
+   data = moose,
+ )
R> round(coef(bin), digits = 4)
\end{CodeInput}
\begin{CodeOutput}
(Intercept)        elev      stratM 
    -0.4247     -0.0003      0.8070 
\end{CodeOutput}
\begin{CodeInput}
R> round(coef(bin_glm), digits = 4)
\end{CodeInput}
\begin{CodeOutput}
(Intercept)        elev      stratM 
    -0.4247     -0.0003      0.8070 
\end{CodeOutput}
\begin{CodeInput}
R> round(sqrt(diag(vcov(bin))), digits = 4)
\end{CodeInput}
\begin{CodeOutput}
(Intercept)        elev      stratM 
     0.4208      0.0019      0.2906 
\end{CodeOutput}
\begin{CodeInput}
R> round(sqrt(diag(vcov(bin_glm))), digits = 4)
\end{CodeInput}
\begin{CodeOutput}
(Intercept)        elev      stratM 
     0.4208      0.0019      0.2906 
\end{CodeOutput}
\end{CodeChunk}

However, using \code{spglm()} instead of \code{glm()} ensures that
\pkg{spmodel} helper functions are available and that each of the
\code{spglm()} models uses the same likelihood:

\begin{CodeChunk}
\begin{CodeInput}
R> glance(spbin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 10
      n     p  npar value   AIC  AICc   BIC logLik deviance pseudo.r.squared
  <int> <dbl> <int> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>
1   218     3     3  676.  682.  683.  693.  -338.     176.           0.0627
\end{CodeOutput}
\begin{CodeInput}
R> glance(bin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 10
      n     p  npar value   AIC  AICc   BIC logLik deviance pseudo.r.squared
  <int> <dbl> <int> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>
1   218     3     0  708.  708.  708.  708.  -354.     294.           0.0280
\end{CodeOutput}
\end{CodeChunk}

The likelihood-based statistics AIC, AICc, BIC, and deviance are much
lower for the SPGLM, indicating a better fit relative to the GLM. We may
also perform a likelihood ratio test (LRT) between the two models, as
the GLM is a special case of the SPGLM (i.e., is nested within the
SPGLM):

\begin{CodeChunk}
\begin{CodeInput}
R> tidy(anova(spbin, bin))
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 5
  full  reduced    df statistic     p.value
  <chr> <chr>   <int>     <dbl>       <dbl>
1 spbin bin         3      31.5 0.000000652
\end{CodeOutput}
\end{CodeChunk}

The LRT test statistic is \(\approx\) 31.5 with three degrees of freedom
(the difference in covariance parameters), yielding a \(p\)-value
\(< 0.001\) that indicates preference for the full model (SPGLM)
relative to the reduced model (GLM).

An alternative approach to model comparison is to use a cross-validation
procedure \citep{james2013introduction}. The \code{loocv()} function
performs leave-one-out cross validation, comparing the predicted mean
(on the response scale) to the observed response variable for each
hold-out observation, recomputing estimates of \(\boldsymbol{\beta}\)
each time. Then, statistics like bias, mean-squared-prediction error
(MSPE), and the square root of MSPE (RMSPE) can be used to evaluate
models:

\begin{CodeChunk}
\begin{CodeInput}
R> loocv(spbin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 3
       bias  MSPE RMSPE
      <dbl> <dbl> <dbl>
1 0.0000206 0.156 0.394
\end{CodeOutput}
\begin{CodeInput}
R> loocv(bin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 3
      bias  MSPE RMSPE
     <dbl> <dbl> <dbl>
1 -1.23e-9 0.240 0.490
\end{CodeOutput}
\end{CodeChunk}

Both models have negligible bias, but the SPGLM has much lower MSPE and
RMSPE than the GLM, indicating the SPGLM predictions are far more
efficient. Three separate metrics (likelihood-based statistics,
likelihood-ratio test, and leave-one-out cross validation) prefer the
SPGLM to the GLM.

We can compare two SPGLMs with different spatial covariance functions
using likelihood-based statistics and leave-one-out cross validation,
but we can't use the LRT because generally, the spatial covariance
functions aren't nested:

\begin{CodeChunk}
\begin{CodeInput}
R> spbin2 <- update(spbin, spcov_type = "spherical")
R> glances(spbin, spbin2)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 2 x 11
  model      n     p  npar value   AIC  AICc   BIC logLik deviance
  <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>
1 spbin2   218     3     3  675.  681.  681.  691.  -338.     180.
2 spbin    218     3     3  676.  682.  683.  693.  -338.     176.
# i 1 more variable: pseudo.r.squared <dbl>
\end{CodeOutput}
\begin{CodeInput}
R> loocv(spbin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 3
       bias  MSPE RMSPE
      <dbl> <dbl> <dbl>
1 0.0000206 0.156 0.394
\end{CodeOutput}
\begin{CodeInput}
R> loocv(spbin2)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 3
      bias  MSPE RMSPE
     <dbl> <dbl> <dbl>
1 0.000121 0.155 0.394
\end{CodeOutput}
\end{CodeChunk}

The \code{"exponential"} spatial covariance (\code{spbin}) has a
slightly lower deviance but slightly higher AIC, AICc, and BIC than the
\code{"spherical"} spatial covariance (\code{spbin2}). Both spatial
covariance functions nearly identical leave-one-out cross validation
metrics. For practical purposes, these models fit quite similarly.

\subsection{Model Diagnostics}\label{model-diagnostics}

\code{spmodel} provides a suite of tools for model diagnostics. The
\code{augment()} function augments the model data with diagnostics:

\begin{CodeChunk}
\begin{CodeInput}
R> augment(spbin)
\end{CodeInput}
\begin{CodeOutput}
Simple feature collection with 218 features and 8 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419057.4 ymax: 1541016
Projected CRS: NAD83 / Alaska Albers
# A tibble: 218 x 9
   presence  elev strat .fitted .resid    .hat  .cooksd .std.resid
 * <fct>    <dbl> <chr>   <dbl>  <dbl>   <dbl>    <dbl>      <dbl>
 1 0         469. L       -1.95 -0.516 0.0476  0.00465      -0.528
 2 0         362. L       -2.70 -0.361 0.0123  0.000548     -0.363
 3 0         173. M       -1.96 -0.514 0.00455 0.000405     -0.516
 4 0         280. L       -3.15 -0.290 0.00413 0.000117     -0.291
 5 0         620. L       -1.19 -0.728 0.168   0.0427       -0.798
 6 0         164. M       -1.71 -0.576 0.00534 0.000598     -0.578
 7 0         164. M       -1.60 -0.606 0.00576 0.000714     -0.608
 8 0         186. L       -2.50 -0.397 0.00439 0.000233     -0.398
 9 0         362. L       -1.88 -0.532 0.0239  0.00237      -0.539
10 0         430. L       -1.54 -0.623 0.0497  0.00713      -0.639
# i 208 more rows
# i 1 more variable: geometry <POINT [m]>
\end{CodeOutput}
\end{CodeChunk}

The fitted values (\code{.fitted}) can be returned on either the link
(\(\hat{\mathbf{w}}\)) or response (\(f^{-1}\hat{\mathbf{w}}\)) scale
and the residuals (\code{.resid}) can deviance, pearson, or response
residuals. The defaults are the link scale and deviance residuals,
respectively. Also returned by \code{augment()} are the leverage
(\code{.hat}), Cook's distance (\code{.cooksd}), and standardized
residuals \code{.std.resid}. A benefit of using \code{augment()} when
\code{data} is an \code{sf} object is that the output is also an
\code{sf} object, which makes it straightforward to create spatial
diagnostic plots (Figure\(~\)\ref{fig-sp-diagnostic}). Standard
\proglang{R} helpers (e.g., \code{fitted()}, \code{residuals()}) are
available to alternatively extract model diagnostics from the model
object.

\begin{figure}
\centering
\includegraphics[width = 1\linewidth]{figures/figure-2.png}
\caption{Spatial logistic regression model diagnostics from [augment]{.fct}. The leverage (i.e., hat) values (left) and standardized residuals (right).}
\label{fig-sp-diagnostic}
\end{figure}

The \code{plot()} function can also be used to return similar
diagnostics as from \code{lm()} and \code{glm()} with additional tools
for spatial covariance. For example, we can inspect Cook's distance
values and the empirical spatial covariance as a function
(Figure\(~\)\ref{fig-sp-diagnostic2}) with

\begin{CodeChunk}
\begin{CodeInput}
R> plot(spbin, which = c(4, 7))
\end{CodeInput}
\end{CodeChunk}

\begin{figure}
\centering
\includegraphics[width = 1\linewidth]{figures/figure-3.png}
\caption{Spatial logistic regression model diagnostics from [plot]{.fct}. The Cook's distance values (left) and the fitted spatial covariance as a function of distance (right).}
\label{fig-sp-diagnostic2}
\end{figure}

The \code{varcomp()} function partitions model variability into several
different components:

\begin{CodeChunk}
\begin{CodeInput}
R> varcomp(spbin)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 3 x 2
  varcomp            proportion
  <chr>                   <dbl>
1 Covariates (PR-sq)   0.0627  
2 de                   0.937   
3 ie                   0.000236
\end{CodeOutput}
\end{CodeChunk}

The pseudo R-squared (\(PR^2\)), the proportion of variability
attributable to the explanatory variables, is reported in the first row.
The remaining variability (\(1 - PR^2\)) is allocated proportionally to
\code{de} and \code{ie} according to \(\sigma^2_{de}\) and
\(\sigma^2_{ie}\). This variability partitioning is a useful that helps
quantify how much the explanatory variables, residual spatial variance,
and residual nonspatial variance contribute to model fit, but as with
\(PR^2\), should not be used as a model comparison tool.

\subsection{Prediction}\label{prediction}

We can predict the probability of moose presence using \code{predict()}:

\begin{CodeChunk}
\begin{CodeInput}
R> predict(spbin, newdata = moose_preds)[1:5]
\end{CodeInput}
\begin{CodeOutput}
          1           2           3           4           5 
 0.06664165 -0.79069107 -1.60387940 -0.83159357  1.38183928 
\end{CodeOutput}
\end{CodeChunk}

By default, predictions are returned on the link scale, but this can be
changed to the response scale via \code{type}:

\begin{CodeChunk}
\begin{CodeInput}
R> predict(spbin, newdata = moose_preds, type = "response")[1:5]
\end{CodeInput}
\begin{CodeOutput}
        1         2         3         4         5 
0.5166542 0.3120203 0.1674401 0.3033082 0.7992862 
\end{CodeOutput}
\end{CodeChunk}

Predictions on the response scale are visualized alongside the fitted
values (\(f^{-1}\hat{\mathbf{w}}\)) in Figure\(~\)\ref{fig-moose-fit}.

\begin{figure}
\centering
\includegraphics[width = 0.70\linewidth]{figures/figure-4.png}
\caption{Moose presence probability fitted values and predictions. Fitted values are represeneted by circles and predictions by triangles.}
\label{fig-moose-fit}
\end{figure}

Prediction intervals are returned via \code{interval}

\begin{CodeChunk}
\begin{CodeInput}
R> predict(spbin, newdata = moose_preds, interval = "prediction")[1:5, ]
\end{CodeInput}
\begin{CodeOutput}
          fit        lwr       upr
1  0.06664165 -2.0374370 2.1707203
2 -0.79069107 -3.4758514 1.8944692
3 -1.60387940 -4.0953329 0.8875741
4 -0.83159357 -3.0704818 1.4072947
5  1.38183928 -0.7692107 3.5328893
\end{CodeOutput}
\end{CodeChunk}

We can alternatively use \code{augment()} to augment the prediction data
with predictions. Arguments to \code{predict()} can also be passed to
\code{augment()}:

\begin{CodeChunk}
\begin{CodeInput}
R> augment(spbin, newdata = moose_preds, interval = "prediction")
\end{CodeInput}
\begin{CodeOutput}
Simple feature collection with 100 features and 5 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 269386.2 ymin: 1418453 xmax: 419976.2 ymax: 1541763
Projected CRS: NAD83 / Alaska Albers
# A tibble: 100 x 6
    elev strat .fitted .lower  .upper           geometry
 * <dbl> <chr>   <dbl>  <dbl>   <dbl>        <POINT [m]>
 1  143. L      0.0666 -2.04   2.17   (401239.6 1436192)
 2  324. L     -0.791  -3.48   1.89   (352640.6 1490695)
 3  158. L     -1.60   -4.10   0.888  (360954.9 1491590)
 4  221. M     -0.832  -3.07   1.41   (291839.8 1466091)
 5  209. M      1.38   -0.769  3.53   (310991.9 1441630)
 6  218. L     -2.59   -5.20   0.0177 (304473.8 1512103)
 7  127. L     -2.73   -5.24  -0.220  (339011.1 1459318)
 8  122. L     -2.32   -4.74   0.0920 (342827.3 1463452)
 9  191  L     -1.17   -4.01   1.66   (284453.8 1502837)
10  105. L     -0.905  -3.05   1.24   (391343.9 1483791)
# i 90 more rows
\end{CodeOutput}
\end{CodeChunk}

By using \code{augment()} when \code{newdata} is an \code{sf} object,
predictions and their corresponding uncertainties are readily available
for spatial mapping (Figure\(~\)\ref{fig-moose-int}).

\begin{figure}
\centering
\includegraphics[width = 1\linewidth]{figures/figure-5.png}
\caption{Moose presence probability prediction intervals. 95\% prediction interval lower bound (left) and 95\% prediction interval upper bound (right).}
\label{fig-moose-int}
\end{figure}

\section{Additional applications}\label{sec-applications2}

Throughout the remainder of this section, we briefly highlight some
additional \pkg{spmodel} capabilities for SPGLMs. In
Section\(~\)\ref{sec-moose-count}, we fit Poisson and negative binomial
models with and without geometric anisotropy for the point-referenced
moose count data. In Section\(~\)\ref{sec-seal}, we fit a binomial model
to the areal seal trend data with a nonspatial random effect. In
Section\(~\)\ref{sec-texas}, we fit beta models to Texas voter turnout
data. We explore how the Texas data can be treated as point-referenced
or areal and compare the two spatial covariance structures empirically.
We also highlight how the maximum likelihood estimation method is useful
when comparing two models with different explanatory variables. Finally,
in Section\(~\)\ref{sec-lake}, we fit a Gamma model to the
point-referenced lake conductivity data. We show how to perform a
spatial analysis of variance (ANOVA) and leverage modeling functions
from other \proglang{R} packages like \pkg{emmeans} and \pkg{car}.

\subsection{Modeling moose counts in Alaska, USA}\label{sec-moose-count}

\begin{CodeChunk}
\begin{CodeInput}
R> sppois <- spglm(
+   formula = count ~ elev + strat,
+   family = poisson,
+   data = moose,
+   spcov_type = "gaussian"
+ )
R> spnb <- update(sppois, family = nbinomial)
R> sppois_anis <- update(sppois, anisotropy = TRUE)
R> spnb_anis <- update(sppois_anis, family = nbinomial)
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> BIC(sppois, spnb, sppois_anis, spnb_anis)
\end{CodeInput}
\begin{CodeOutput}
            df      BIC
sppois       3 1343.892
spnb         4 1340.706
sppois_anis  5 1337.610
spnb_anis    6 1335.051
\end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> plot(spnb, which = 8)
R> plot(spnb_anis, which = 8)
\end{CodeInput}
\end{CodeChunk}

\subsection{Modeling harbor seal trends in Alaska, USA}\label{sec-seal}

\begin{CodeChunk}
\begin{CodeInput}
R> spbin <- spgautor(
+   formula = log_trend > 0 ~ 1,
+   family = binomial,
+   data = seal,
+   spcov_type = "car",
+   random = ~ stock
+ )
R> tidy(spbin, conf.int = TRUE)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 7
  term        estimate std.error statistic p.value conf.low conf.high
  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>
1 (Intercept)   -0.340     0.673    -0.506   0.613    -1.66     0.979
\end{CodeOutput}
\end{CodeChunk}

\subsection{Modeling voter turnout in Texas, USA}\label{sec-texas}

\begin{CodeChunk}
\begin{CodeInput}
R> spbeta_geo <- spglm(
+   formula = turnout ~ log_income,
+   family = "beta", 
+   data = texas,
+   spcov_type = "matern"
+ )
R> 
R> spbeta_auto <- spgautor(
+   formula = turnout ~ log_income,
+   family = "beta", 
+   data = texas,
+   spcov_type = "car",
+   cutoff = 1e5
+ )
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> AIC(spbeta_geo, spbeta_auto)
\end{CodeInput}
\begin{CodeOutput}
            df       AIC
spbeta_geo   5 -44.53113
spbeta_auto  3 -22.46104
\end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> tidy(spbeta_geo)
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 2 x 5
  term        estimate std.error statistic    p.value
  <chr>          <dbl>     <dbl>     <dbl>      <dbl>
1 (Intercept)   -5.20      1.11      -4.70 0.00000260
2 log_income     0.579     0.122      4.76 0.00000194
\end{CodeOutput}
\begin{CodeInput}
R> spbeta_full_ml <- update(spbeta_geo, estmethod = "ml")
R> spbeta_red_ml <- update(spbeta_full_ml, formula = turnout ~ 1)
R> tidy(anova(spbeta_full_ml, spbeta_red_ml))
\end{CodeInput}
\begin{CodeOutput}
# A tibble: 1 x 5
  full          reduced           df statistic    p.value
  <chr>         <chr>          <dbl>     <dbl>      <dbl>
1 spbeta_red_ml spbeta_full_ml     1      23.2 0.00000149
\end{CodeOutput}
\end{CodeChunk}

\subsection{Modeling lake conductivity in Southwest,
USA}\label{sec-lake}

\begin{CodeChunk}
\begin{CodeInput}
R> spgam <- spglm(
+   formula = exp(log_cond) ~ temp * state + origin, 
+   family = "Gamma",
+   data = lake,
+   spcov_type = "cauchy",
+   partition_factor = ~ year
+ )
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> anova(spgam)
\end{CodeInput}
\begin{CodeOutput}
Analysis of Variance Table

Response: exp(log_cond)
            Df    Chi2 Pr(>Chi2)    
(Intercept)  1 51.5270 7.062e-13 ***
temp         1 25.5146 4.390e-07 ***
state        3  3.0747 0.3802528    
origin       1  0.1429 0.7053819    
temp:state   3 19.7668 0.0001897 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> car::vif(spgam)
\end{CodeInput}
\begin{CodeOutput}
                 GVIF Df GVIF^(1/(2*Df))
temp         4.691914  1        2.166083
state      127.082397  3        2.242234
origin       1.264940  1        1.124695
temp:state  76.387383  3        2.059856
\end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> pairs(emmeans::emmeans(spgam, ~ state | temp))
\end{CodeInput}
\begin{CodeOutput}
temp = 7.63:
 contrast estimate    SE  df z.ratio p.value
 AZ - CO    -1.012 0.337 Inf  -3.004  0.0142
 AZ - NV    -0.900 0.348 Inf  -2.584  0.0480
 AZ - UT    -1.331 0.326 Inf  -4.082  0.0003
 CO - NV     0.112 0.258 Inf   0.434  0.9727
 CO - UT    -0.319 0.223 Inf  -1.427  0.4822
 NV - UT    -0.431 0.244 Inf  -1.763  0.2915

Results are averaged over the levels of: origin 
Degrees-of-freedom method: asymptotic 
Results are given on the log (not the response) scale. 
P value adjustment: tukey method for comparing a family of 4 estimates 
\end{CodeOutput}
\begin{CodeInput}
R> emmeans::emtrends(spgam, ~ state, var = "temp")
\end{CodeInput}
\begin{CodeOutput}
 state temp.trend     SE  df asymp.LCL asymp.UCL
 AZ         0.152 0.0301 Inf    0.0929     0.211
 CO         0.289 0.0370 Inf    0.2161     0.361
 NV         0.171 0.0504 Inf    0.0718     0.270
 UT         0.352 0.0372 Inf    0.2791     0.425

Results are averaged over the levels of: origin 
Degrees-of-freedom method: asymptotic 
Results are given on the exp (not the response) scale. 
Confidence level used: 0.95 
\end{CodeOutput}
\end{CodeChunk}

\section{Discussion}\label{discussion}

\bibliography{references.bib}




\end{document}
